{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyoung000/summanews/blob/main/summanews_server(Gemini%20API%2BHugging_Face).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flask flask-cors requests beautifulsoup4 pyngrok lxml newspaper3k lxml_html_clean transformers torch sentencepiece scikit-learn trafilatura\n",
        "!pip install -U newspaper3k\n",
        "\n",
        "print(\"ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ! (ì‹œê°„ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)\")\n",
        "\n",
        "# -----------------------------\n",
        "# ì„œë²„ ë° í™˜ê²½ ì„¤ì •\n",
        "# -----------------------------\n",
        "from flask import Flask, jsonify, request, render_template_string\n",
        "from flask_cors import CORS\n",
        "import os, requests, json, time, re, random, threading\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "from newspaper import Article\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import concurrent.futures\n",
        "# âš¡ï¸ Trafilatura ì„í¬íŠ¸\n",
        "import trafilatura\n",
        "\n",
        "# --- Gemini API ì„¤ì • ---\n",
        "# âš ï¸ ì‚¬ìš©ìë‹˜ì´ ì œê³µí•œ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "GEMINI_API_KEY = \"AIzaSyAszdb8T_H0BhrGd69ZLaD2uZjcU3KtkQE\"\n",
        "# âš¡ï¸ GEMINI_THRESHOLD ë³€ìˆ˜ ì œê±° (í† í° ìˆ˜ë¡œ ëŒ€ì²´)\n",
        "GEMINI_MODEL_NAME = \"gemini-2.5-flash\"\n",
        "# ---------------------\n",
        "\n",
        "# --- ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# ---\n",
        "\n",
        "# Colab ë¹„ë™ê¸° ê´€ë ¨\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì‹¤ì œ ê°’ì€ ìˆ¨ê¹€)\n",
        "os.environ.setdefault('NAVER_CLIENT_ID', 'fe9DLGhYbEVLy4sdQnVk')\n",
        "os.environ.setdefault('NAVER_CLIENT_SECRET', '2f0NEntTNN')\n",
        "os.environ.setdefault('NEWSAPI_KEY', 'a80b5826f01349c5824f4298d8f61eef')\n",
        "NGROK_AUTHTOKEN = \"30KyKWx0zSS7ZJpm5TnJOKdI6fC_7y1Lta8QCMEyH6ZLjjrxj\"\n",
        "\n",
        "# âš¡ï¸ ì¹´í…Œê³ ë¦¬ ìœ ì§€: 'ì„¸ê³„', 'ë‚ ì”¨' ìœ ì§€\n",
        "CATEGORIES = {\n",
        "    'ì •ì¹˜': 'politics',\n",
        "    'ê²½ì œ': 'business',\n",
        "    'ì‚¬íšŒ': 'society',\n",
        "    'ìƒí™œë¬¸í™”': 'life',\n",
        "    'ì—°ì˜ˆ': 'entertainment',\n",
        "    'ìŠ¤í¬ì¸ ': 'sports',\n",
        "    'ITê³¼í•™': 'technology',\n",
        "    'ì„¸ê³„': 'world',\n",
        "    'ë‚ ì”¨': 'weather',\n",
        "    'ì˜¤ëŠ˜ì˜ì¶”ì²œ': 'today'\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# âœ… KoBERT/T5 ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "# -----------------------------\n",
        "def init_summarizer():\n",
        "    \"\"\"KoBERT/T5 ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì»´í¬ë„ŒíŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    print(\"KoBERT/T5 ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!)\")\n",
        "    candidates = [\n",
        "        \"lcw99/t5-base-korean-text-summary\",\n",
        "        \"gogamza/kobart-summarization\",\n",
        "    ]\n",
        "    for model_name in candidates:\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "            pipeline_obj = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"pt\")\n",
        "\n",
        "            eos_token_id = tokenizer.eos_token_id\n",
        "            if eos_token_id is None:\n",
        "                eos_token_id = tokenizer.convert_tokens_to_ids(\"</s>\") if \"</s>\" in tokenizer.get_vocab() else 1\n",
        "\n",
        "            print(f\"âœ… KoBERT/T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}, EOS Token ID: {eos_token_id}\")\n",
        "            return {'pipeline': pipeline_obj, 'tokenizer': tokenizer, 'model': model, 'eos_token_id': eos_token_id}\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({model_name}): {e}\")\n",
        "            continue\n",
        "    print(\"âš ï¸ ëª¨ë“  KoBERT/T5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ë£° ê¸°ë°˜ ìš”ì•½(fallback)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "    return None\n",
        "\n",
        "summarizer_components = init_summarizer()\n",
        "\n",
        "# -----------------------------\n",
        "# âš¡ï¸ Gemini API í˜¸ì¶œ í•¨ìˆ˜ (ê¸´ ê¸°ì‚¬ìš©)\n",
        "# -----------------------------\n",
        "def call_gemini_api(text):\n",
        "    \"\"\"ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ Gemini APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "    # âš¡ï¸ ìµœì¢… í”„ë¡¬í”„íŠ¸ ì ìš© (ë¬¸ì¥ ì™„ì„± ë° ìœ ì°½ì„± ê°•ì¡°)\n",
        "    # íŒŒì´ì¬ì˜ ë¬¸ìì—´ ì—°ê²°ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°€ë…ì„±ì„ ìœ ì§€í•˜ë©´ì„œ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ì „ë‹¬\n",
        "    system_prompt = (\n",
        "        \"ë„ˆëŠ” ë‰´ìŠ¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•´ì„œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ ìš”ì•½ê°€ì•¼. \"\n",
        "        \"ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ì˜¤ì§ ì‚¬ì‹¤ê³¼ í•µì‹¬ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì™„ë²½í•˜ê²Œ íŒŒì•…í•´. \"\n",
        "        \"íŒŒì•…í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 300ì ì´ë‚´ì˜ ê°„ê²°í•˜ê³  ìœ ìµí•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´. \"\n",
        "        \"ìš”ì•½ë¬¸ì€ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•˜ë©°, ë‹¨ì–´ ë‹¨ìœ„ë¡œ ëë‚˜ì§€ ì•Šë„ë¡ ìœ ì˜í•´. \"\n",
        "        \"ìš”ì•½ë¬¸ë§Œ ì¶œë ¥í•´ì•¼ í•˜ë©°, ì œëª©, ì„œë¡ ì ì¸ ë¬¸êµ¬, ê°ìƒ ë“± ì–´ë– í•œ ë¶€ê°€ì ì¸ í…ìŠ¤íŠ¸ë„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ.\"\n",
        "    )\n",
        "    user_query = text # ê¸°ì‚¬ ë‚´ìš© ìì²´ë§Œ ë³´ëƒ„\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_query}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"config\": {\n",
        "            # 300ê¸€ì ì´í•˜ë¥¼ ëª©í‘œë¡œ maxOutputTokensë¥¼ 160 í† í°ìœ¼ë¡œ ìœ ì§€\n",
        "            \"maxOutputTokens\": 160,\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # 20ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì„¤ì •\n",
        "            response = requests.post(url, headers={'Content-Type': 'application/json'}, json=payload, timeout=20)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "\n",
        "            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "            candidate = result.get('candidates', [{}])[0]\n",
        "            if candidate and candidate.get('content') and candidate['content'].get('parts'):\n",
        "                summary = candidate['content']['parts'][0].get('text', '').strip()\n",
        "                if summary:\n",
        "                    return summary\n",
        "\n",
        "            raise ValueError(\"Gemini APIê°€ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt\n",
        "                # print(f\"Gemini API ìš”ì²­ ì‹¤íŒ¨. {wait_time}ì´ˆ í›„ ì¬ì‹œë„...\") # ë””ë²„ê¹…ìš©\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                # print(f\"Gemini API ìµœì¢… ì‹¤íŒ¨: {e}\") # ë””ë²„ê¹…ìš©\n",
        "                raise e\n",
        "        except Exception as e:\n",
        "             # ê¸°íƒ€ íŒŒì‹± ì˜¤ë¥˜ë‚˜ ValueError ì²˜ë¦¬\n",
        "             raise e\n",
        "\n",
        "    return \"\" # ìµœì¢…ì ìœ¼ë¡œ ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜\n",
        "\n",
        "# -----------------------------\n",
        "# ê¸°ì‚¬ ë‚´ìš© ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# -----------------------------\n",
        "def clean_text(html_text):\n",
        "    \"\"\"HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ê³µë°±ì„ ì •ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'<[^>]+>', '', html_text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def get_article_text(url):\n",
        "    \"\"\"\n",
        "    ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. Trafilaturaë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³  ì‹¤íŒ¨ ì‹œ newspaper3kë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°,\n",
        "    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    text = None\n",
        "\n",
        "    try:\n",
        "        # 1. âš¡ï¸ Trafilatura ì‹œë„ (ë” ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€)\n",
        "        downloaded = trafilatura.fetch_url(url)\n",
        "        if downloaded:\n",
        "            text = trafilatura.extract(\n",
        "                downloaded,\n",
        "                favor_recall=True,          # ìµœëŒ€í•œ ë§ì€ ë‚´ìš©ì„ ì¶”ì¶œ (í•œêµ­ì–´ ê¸°ì‚¬ì— ìœ ë¦¬)\n",
        "                include_comments=False,     # ëŒ“ê¸€ ì œì™¸\n",
        "                target_language='ko',\n",
        "                output_format='json',       # JSONìœ¼ë¡œ ë°›ì•„ 'text' í•„ë“œë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì •ì \n",
        "            )\n",
        "            if text:\n",
        "                try:\n",
        "                    text_json = json.loads(text)\n",
        "                    # titleë„ í…ìŠ¤íŠ¸ê°€ ì§§ê±°ë‚˜ ì—†ì„ ê²½ìš° ë³´ì¡°ìš©ìœ¼ë¡œ ê°€ì ¸ì˜´\n",
        "                    text = text_json.get('text', '') or text_json.get('title', '')\n",
        "                except json.JSONDecodeError:\n",
        "                    # JSON ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ì‹œ í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
        "                    pass\n",
        "\n",
        "        # 2. Trafilatura ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ newspaper3k í´ë°±\n",
        "        if not text or len(text) < 100:\n",
        "            article = Article(url, language='ko')\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            text = article.text\n",
        "\n",
        "        if not text:\n",
        "            return None\n",
        "\n",
        "        # 3. í…ìŠ¤íŠ¸ í´ë¦¬ë‹ ê°•í™” (ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë‘ì—ê²Œ ì ìš©)\n",
        "\n",
        "        # ê¸°ì ì •ë³´ ë° HTML íƒœê·¸ì„± ë‚´ìš©, ê´‘ê³ ì„± ë¬¸êµ¬ ì œê±° ê°•í™”\n",
        "        patterns_to_remove = [\n",
        "            r'\\[.*?\\]', r'\\s*\\(.*?=.*?\\)\\s*', r'[ê°€-í£]{2,4} ê¸°ì',\n",
        "            r'ê¸°ì = [ê°€-í£]{2,4}', r'â“’\\s*.*?ë¬´ë‹¨ì „ì¬\\s*ë°\\s*ì¬ë°°í¬\\s*ê¸ˆì§€',\n",
        "            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', r'ë‹¤ì‹œë³´ê¸°',\n",
        "            r'\\([^)]*=\\s*\\w+@\\w+\\.\\w+\\)', # (ì´ë¦„=ì´ë©”ì¼) í˜•íƒœ ì œê±°\n",
        "            r'ì‚¬ì§„=.*?ì œê³µ', r'[\\s\\n]*\\(ì¶œì²˜:.*?\\)', # ì‚¬ì§„ ì¶œì²˜ ì œê±°\n",
        "            r'â–¶\\s*ë°”ë¡œê°€ê¸°.*', r'â€»\\s*.*\\s*ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€',\n",
        "            r'ì €ì‘ê¶Œì\\(c\\).*', r'Copyright.*',\n",
        "            r'[ê°€-í£]{2,4}ì´/ê°€ [ê°€-í£]{2,4}ì—ê²Œ ë“œë¦¬ëŠ” í•œë§ˆë””',\n",
        "            r'[ê°€-í£]{2,4} [ê°€-í£]{2,4} ê¸°ì[=.]\\s*.*?\\]',\n",
        "            r'â–²\\s*.*',           # â–²ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì¤„ ì œê±° (ì´ë¯¸ì§€ ì„¤ëª…/ì •ë³´)\n",
        "            r'Â©',                # Â© ê¸°í˜¸ ì œê±°\n",
        "            r'\\[ì‚¬ì§„\\s*.*?\\]',    # [ì‚¬ì§„=ì—°í•©ë‰´ìŠ¤] ì œê±°\n",
        "            r'\\[ì´ë¯¸ì§€ì¶œì²˜.*?\\]', # [ì´ë¯¸ì§€ì¶œì²˜=OOO] ì œê±°\n",
        "            r'[â– â—†â—]',           # âš¡ï¸ Fix 2: ëª©ë¡ ê¸°í˜¸ ì œê±°\n",
        "            r'\\s*[ê°€-í£]{2,4}\\s*(íŠ¹íŒŒì›|ê¸°ì)\\s*=\\s*', # âš¡ï¸ Fix 2: 'ìµœì§„ìš° íŠ¹íŒŒì› =' í˜•íƒœ ì œê±°\n",
        "            r'^\\s*\\[(ë‹¨ë…|ì¢…í•©|ì†ë³´|ì•µì»¤|ì˜ìƒ)\\]\\s*', # âš¡ï¸ ìƒˆ íŒ¨í„´: [ë‹¨ë…] ë“± ë¨¸ë¦¬ë§ ì œê±°\n",
        "            r'\\([a-zA-Zê°€-í£0-9]+\\s*=\\s*.*?\\)|\\([a-zA-Zê°€-í£0-9]+\\)\\s*', # âš¡ï¸ ì„¤ë¦¬ ê¸°ì‚¬ ë¬¸ì œ í•´ê²°: (ìµœì§„ë¦¬)ë‚˜ (f(x) ì„¤ë¦¬) ê°™ì€ ê´„í˜¸ ì•ˆì˜ ë©”íƒ€ë°ì´í„° ì œê±°\n",
        "            r'^\\s*[\\(\\[=]*\\s*[ê°€-í£\\s/]*\\s*[=]*\\s*', # âš¡ï¸ ê¸°ì‚¬ ì‹œì‘ ì‹œ ë¶™ëŠ” íŠ¹ìˆ˜ ê¸°í˜¸/ì¶œì²˜ í‘œê¸°/ê¸°ì ì •ë³´ ë“± ì œê±° (ê°€ì¥ ê°•ë ¥í•œ ì‹œì‘ í´ë¦¬ë‹)\n",
        "        ]\n",
        "        for pattern in patterns_to_remove:\n",
        "            text = re.sub(pattern, '', text).strip()\n",
        "\n",
        "        # âš¡ï¸ Fix 5: ë³¸ë¬¸ ì‹œì‘ì— ë¶™ëŠ” ì¶œì²˜ í‘œê¸° (ì˜ˆ: (ì œì£¼/êµ­ì œë‰´ìŠ¤) = ) ì œê±°\n",
        "        text = re.sub(r'^\\s*\\(.*?\\)\\s*=\\s*', '', text).strip()\n",
        "\n",
        "        # ê¸°ì‚¬ ê¼¬ë¦¬ë§/ì •ë¦¬ ë¬¸êµ¬ ì œê±° (ë¬¸ì¥ ë‹¨ìœ„ë¡œ í›„ì²˜ë¦¬)\n",
        "        ending_patterns_to_remove = [\n",
        "            r'\\(ë\\)\\s*', r'\\(ìë£Œì‚¬ì§„=.*?ì—°í•©ë‰´ìŠ¤\\)',\n",
        "            r'ì§€ê¸ˆê¹Œì§€ [ê°€-í£]{2,4}ì˜€ìŠµë‹ˆë‹¤.',\n",
        "            r'ë³¸ ê¸°ì‚¬ëŠ” [ê°€-í£]{2,4}ì˜ í—ˆë½ í•˜ì— [ê°€-í£]{2,4}ì—ì„œ ì¬í¸ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.',\n",
        "            r'ìì„¸í•œ ë‚´ìš©ì€ [ê°€-í£]{2,4} [ê°€-í£]{2,4}ì—ì„œ í™•ì¸í•˜ì„¸ìš”.',\n",
        "            r'\\s*\\(ìë£Œì‚¬ì§„\\)', r'â–¶.*', r'\\[[ê°€-í£]{2,4} í•œë§ˆë””\\]',\n",
        "            r'\\([ê°€-í£]{2,4}\\s*ë‰´ìŠ¤\\)', r'\\([ê°€-í£]{2,4}=\\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\)'\n",
        "        ]\n",
        "\n",
        "        sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', text)\n",
        "        cleaned_sentences = sentences[:]\n",
        "\n",
        "        if len(sentences) > 5:\n",
        "            removed_count = 0\n",
        "            for i in range(1, min(6, len(sentences))):\n",
        "                last_sentence = cleaned_sentences[-1] if cleaned_sentences else \"\"\n",
        "\n",
        "                is_ending_pattern = False\n",
        "                for pattern in ending_patterns_to_remove:\n",
        "                    if re.search(pattern, last_sentence):\n",
        "                        is_ending_pattern = True\n",
        "                        break\n",
        "\n",
        "                # ì§§ê³  í”í•œ ë¬¸êµ¬(ì‚¬ì§„, ì œê³µ ë“±)ë§Œ í¬í•¨í•˜ëŠ” ê²½ìš° ì œê±°\n",
        "                is_junk_phrase = re.search(r'(ì‚¬ì§„|ì œê³µ|ë°°í¬|ë‰´ìŠ¤|ê¸°ì|ë°”ë¡œê°€ê¸°|í›„ì›)', last_sentence) and len(last_sentence) < 40\n",
        "\n",
        "                if is_ending_pattern or is_junk_phrase or (len(last_sentence) < 30 and len(cleaned_sentences) > 5 and removed_count < 3):\n",
        "                    cleaned_sentences.pop()\n",
        "                    removed_count += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        text = \" \".join(cleaned_sentences)\n",
        "\n",
        "        # 4. ìµœì¢…ì ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"í¬ë¡¤ë§ ì˜¤ë¥˜ ë°œìƒ: {e}\") # ë””ë²„ê¹…ìš©\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# âœ… ìš”ì•½ í•¨ìˆ˜ (KoBERT/T5 + Gemini í•˜ì´ë¸Œë¦¬ë“œ)\n",
        "# -----------------------------\n",
        "def make_summary(text):\n",
        "    \"\"\"ìš”ì•½ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ ë£° ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\"\"\"\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # í•œì ë° íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±° ê°•í™”\n",
        "    text = re.sub(r'[\\u4E00-\\u9FFF]+', '', text)\n",
        "    text = re.sub(r'\\([ê°€-í£]{2,4}=\\w+\\)', '', text)\n",
        "    text = re.sub(r'\\([ê°€-í£]{2,4} ê¸°ì\\)', '', text).strip()\n",
        "    text = re.sub(r'\\[.*\\]', '', text)\n",
        "\n",
        "    summarizer_active = False\n",
        "\n",
        "    if summarizer_components:\n",
        "        tokenizer = summarizer_components['tokenizer']\n",
        "        # âš¡ï¸ KoBERT/T5 ëª¨ë¸ì˜ ì…ë ¥ í† í° ê¸¸ì´ ê³„ì‚°\n",
        "        tokenized_input = tokenizer(text, truncation=False, return_tensors=\"pt\")\n",
        "        input_length = tokenized_input['input_ids'].size(1)\n",
        "\n",
        "        # 1. âš¡ï¸ Gemini API í˜¸ì¶œ (ê¸´ ê¸°ì‚¬: 1024 í† í° ì´ˆê³¼ ì‹œ)\n",
        "        # KoBERT/T5ì˜ ìµœëŒ€ ì…ë ¥ í† í° ìˆ˜(1024)ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸\n",
        "        if input_length > 1024:\n",
        "            try:\n",
        "                print(f\"ğŸš€ ê¸´ ê¸°ì‚¬ì…ë‹ˆë‹¤ ({input_length} í† í°). Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.\")\n",
        "                summary = call_gemini_api(text)\n",
        "                if summary:\n",
        "                    summarizer_active = True\n",
        "                    # ë§ˆì¹¨í‘œ ì²˜ë¦¬\n",
        "                    if not summary.endswith(('.', '!', '?', 'â€¦')):\n",
        "                        summary += '.'\n",
        "                    return summary\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Gemini API ì¶”ë¡  ì‹¤íŒ¨. KoBERT/T5 í´ë°±ì„ ì‹œë„í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}\")\n",
        "                pass # Gemini ì‹¤íŒ¨ ì‹œ KoBERTë¡œ í´ë°±\n",
        "\n",
        "        # 2. KoBERT/T5 ëª¨ë¸ í˜¸ì¶œ (ì§§ê±°ë‚˜ ì¤‘ê°„ ê¸°ì‚¬: 100ì ì´ìƒ, 1024 í† í° ì´í•˜)\n",
        "        # 100ì ì´ìƒì´ê³ , 1024 í† í° ì´í•˜ì¼ ê²½ìš° KoBERT/T5 ì‚¬ìš©\n",
        "        if len(text) >= 100 and input_length <= 1024:\n",
        "            try:\n",
        "                print(f\"ğŸ§  ì§§ê±°ë‚˜ ì¤‘ê°„ ê¸¸ì´ ê¸°ì‚¬ì…ë‹ˆë‹¤ ({input_length} í† í°). KoBERT/T5ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.\")\n",
        "                model = summarizer_components['model']\n",
        "                eos_token_id = summarizer_components['eos_token_id']\n",
        "\n",
        "                # ê¸´ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° ìë¥´ê¸° (íŠ¸ë ì¼€ì´ì…˜=Trueë¡œ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ)\n",
        "                tokenized_input_trunc = tokenizer(text, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "                # KoBERT/T5 ì„¤ì • (300ê¸€ì ì´í•˜, ìœ ì°½í•œ ìš”ì•½ ìœ ë„)\n",
        "                summary_ids = model.generate(\n",
        "                    tokenized_input_trunc['input_ids'],\n",
        "                    max_length=180,        # 300 ê¸€ì ì´í•˜ë¥¼ ëª©í‘œë¡œ 180 í† í°ìœ¼ë¡œ ì œí•œ\n",
        "                    min_length=min(100, input_length // 2), # ì…ë ¥ í† í°ì˜ ì ˆë°˜ ë˜ëŠ” 100 ì¤‘ ì‘ì€ ê°’\n",
        "                    num_beams=5,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    repetition_penalty=1.5, # âš¡ï¸ ì•ˆì •ì„± ì¬í™•ë³´: ë°˜ë³µ í˜ë„í‹° ì¬ë„ì…\n",
        "                    length_penalty=1.0,     # âš¡ï¸ ì•ˆì •ì„± ì¬í™•ë³´: ì¤‘ë¦½ì ì¸ ê¸¸ì´ í˜ë„í‹° ì¬ë„ì…\n",
        "                    do_sample=False,\n",
        "                    forced_eos_token_id=eos_token_id,\n",
        "                )\n",
        "\n",
        "                summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True).strip()\n",
        "\n",
        "                if summary:\n",
        "                    summarizer_active = True\n",
        "                    # ë§ˆì¹¨í‘œ ì²˜ë¦¬\n",
        "                    if not summary.endswith(('.', '!', '?', 'â€¦')):\n",
        "                        summary += '.'\n",
        "                    return summary\n",
        "\n",
        "                raise ValueError(\"KoBERT/T5 ëª¨ë¸ì´ ìš”ì•½ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ KoBERT/T5 ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜ ë°œìƒ. ë£° ê¸°ë°˜ í´ë°±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}\")\n",
        "                pass # KoBERT ì‹¤íŒ¨ ì‹œ Fallbackìœ¼ë¡œ ì´ë™\n",
        "\n",
        "    # 3. ë£° ê¸°ë°˜ Fallback ìš”ì•½ (100ì ë¯¸ë§Œ ë˜ëŠ” ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œ)\n",
        "    print(\"ğŸ”» ëª¨ë“  ëª¨ë¸ ìš”ì•½ ì‹¤íŒ¨ ë˜ëŠ” 100ì ë¯¸ë§Œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë£° ê¸°ë°˜ Fallbackì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "    sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', text)\n",
        "\n",
        "    # 3ê°œ ë¬¸ì¥ë§Œ ê°€ì ¸ì™€ì„œ ìµœëŒ€ 500ìë¡œ ì œí•œ (ì§§ì€ ì „ë¬¸ ë…¸ì¶œ ë°©ì§€)\n",
        "    fallback_summary = \" \".join(sentences[:3])\n",
        "\n",
        "    # ì—¬ê¸°ì„œ ìµœì¢…ì ì¸ ìµœì•…ì˜ ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬\n",
        "    if fallback_summary == \"ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒì„¸ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\":\n",
        "        fallback_summary = \"ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒì„¸ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\"\n",
        "    elif len(fallback_summary) > 500:\n",
        "        fallback_summary = fallback_summary[:500] + \"...\"\n",
        "    elif not fallback_summary.endswith(('.', '!', '?', 'â€¦')):\n",
        "        fallback_summary += \".\"\n",
        "\n",
        "    return fallback_summary\n",
        "\n",
        "\n",
        "def extract_image_and_views(url, timeout=6):\n",
        "    \"\"\"ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ URLê³¼ ì¡°íšŒìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "        if resp.status_code != 200:\n",
        "            return None, None\n",
        "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "        img_selectors = [\n",
        "            ('meta[property=\"og:image\"]', 'content'), ('meta[name=\"twitter:image\"]', 'content'),\n",
        "            ('meta[itemprop=\"image\"]', 'content'), ('img[class*=\"thumb\"]', 'src'),\n",
        "            ('img[class*=\"photo\"]', 'src'), ('article img', 'src'), ('img', 'src')\n",
        "        ]\n",
        "        image = None\n",
        "        for sel, attr in img_selectors:\n",
        "            tag = soup.select_one(sel)\n",
        "            if tag:\n",
        "                image = tag.get(attr) if getattr(tag, 'get', None) else None\n",
        "                if image and image.startswith('//'):\n",
        "                    image = 'https:' + image\n",
        "                if image:\n",
        "                    break\n",
        "\n",
        "        text = soup.get_text(separator=' ')\n",
        "        view_patterns = [r'ì¡°íšŒìˆ˜[^\\d]*([\\d,]+)', r'ì¡°íšŒ[^\\d]*([\\d,]+)', r'Views[^\\d]*([\\d,]+)', r'views[^\\d]*([\\d,]+)', r'view[^\\d]*([\\d,]+)', r'ì½ìŒ[^\\d]*([\\d,]+)', r'PV[^\\d]*([\\d,]+)']\n",
        "        views = None\n",
        "        for p in view_patterns:\n",
        "            m = re.search(p, text, re.IGNORECASE)\n",
        "            if m:\n",
        "                v = m.group(1)\n",
        "                v = int(re.sub(r'[^0-9]', '', v))\n",
        "                views = v\n",
        "                break\n",
        "\n",
        "        if not image:\n",
        "            try:\n",
        "                art = Article(url)\n",
        "                art.download()\n",
        "                art.parse()\n",
        "                if art.top_image:\n",
        "                    image = art.top_image\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return image, views\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def extract_publisher_name(url):\n",
        "    \"\"\"URLì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë„¤ì´ë²„ API OIDì™€ ë„ë©”ì¸ ë§¤í•‘ì„ í™œìš©í•©ë‹ˆë‹¤.\"\"\"\n",
        "    try:\n",
        "        publisher_map = {\n",
        "            'chosun.com': 'ì¡°ì„ ì¼ë³´', 'donga.com': 'ë™ì•„ì¼ë³´', 'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',\n",
        "            'hankyung.com': 'í•œêµ­ê²½ì œ', 'maeil.com': 'ë§¤ì¼ê²½ì œ', 'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',\n",
        "            'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸', 'pressian.com': 'í”„ë ˆì‹œì•ˆ', 'etoday.co.kr': 'ì´íˆ¬ë°ì´',\n",
        "            'shinailbo.co.kr': 'ì‹ ì•„ì¼ë³´', 'news1.kr': 'ë‰´ìŠ¤1', 'inews24.com': 'ì•„ì´ë‰´ìŠ¤24',\n",
        "            'it.chosun.com': 'ITì¡°ì„ ', 'ddaily.co.kr': 'ë””ì§€í„¸ë°ì¼ë¦¬', 'zdnet.co.kr': 'ì§€ë””ë„·ì½”ë¦¬ì•„',\n",
        "            'bloter.net': 'ë¸”ë¡œí„°', 'edaily.co.kr': 'ì´ë°ì¼ë¦¬', 'fnnews.com': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',\n",
        "            'kbs.co.kr': 'KBS', 'hani.co.kr': 'í•œê²¨ë ˆ', 'mk.co.kr': 'ë§¤ì¼ê²½ì œ',\n",
        "        }\n",
        "\n",
        "        match = re.search(r'https?://(?:www\\.)?([^/]+)', url)\n",
        "        extracted_domain = match.group(1) if match else None\n",
        "\n",
        "        if extracted_domain and extracted_domain in publisher_map:\n",
        "            return publisher_map[extracted_domain]\n",
        "\n",
        "        if \"naver.com\" in url or \"daum.net\" in url:\n",
        "            oid_map = {\n",
        "                '001': 'ì—°í•©ë‰´ìŠ¤', '003': 'ì¤‘ì•™ì¼ë³´', '005': 'êµ­ë¯¼ì¼ë³´', '008': 'ë¨¸ë‹ˆíˆ¬ë°ì´',\n",
        "                '009': 'ë§¤ì¼ê²½ì œ', '011': 'ì„œìš¸ê²½ì œ', '014': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', '018': 'ì´ë°ì¼ë¦¬',\n",
        "                '020': 'ë™ì•„ì¼ë³´', '021': 'ë¬¸í™”ì¼ë³´', '023': 'ì¡°ì„ ì¼ë³´', '025': 'ì¤‘ì•™ì¼ë³´',\n",
        "                '028': 'í•œê²¨ë ˆ', '032': 'ê²½í–¥ì‹ ë¬¸'\n",
        "            }\n",
        "            match = re.search(r'oid=([0-9]+)', url)\n",
        "            if match:\n",
        "                oid = match.group(1)\n",
        "                return oid_map.get(oid, 'ë„¤ì´ë²„ ë‰´ìŠ¤')\n",
        "            return 'ë„¤ì´ë²„ ë‰´ìŠ¤'\n",
        "\n",
        "        return extracted_domain if extracted_domain else \"ì•Œ ìˆ˜ ì—†ìŒ\"\n",
        "    except Exception as e:\n",
        "        return \"ì•Œ ìˆ˜ ì—†ìŒ\"\n",
        "\n",
        "# -----------------------------\n",
        "# ë‰´ìŠ¤ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤\n",
        "# -----------------------------\n",
        "class NewsService:\n",
        "    \"\"\"ë„¤ì´ë²„ ë° NewsAPIë¡œë¶€í„° ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” í´ë˜ìŠ¤.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.naver_client_id = os.environ.get('NAVER_CLIENT_ID')\n",
        "        self.naver_client_secret = os.environ.get('NAVER_CLIENT_SECRET')\n",
        "        self.newsapi_key = os.environ.get('NEWSAPI_KEY')\n",
        "        # âš¡ï¸ query_map ì—…ë°ì´íŠ¸: 'ì„¸ê³„', 'ë‚ ì”¨' ìœ ì§€\n",
        "        self.query_map = {\n",
        "            'ì •ì¹˜': 'ì •ì¹˜', 'ê²½ì œ': 'ê²½ì œ', 'ì‚¬íšŒ': 'ì‚¬íšŒ', 'ìƒí™œë¬¸í™”': 'ìƒí™œ ë¬¸í™”', 'ì—°ì˜ˆ': 'ì—°ì˜ˆ', 'ìŠ¤í¬ì¸ ': 'ìŠ¤í¬ì¸ ',\n",
        "            'ITê³¼í•™': 'ITê³¼í•™', 'ì„¸ê³„': 'ì„¸ê³„ ë‰´ìŠ¤', 'ë‚ ì”¨': 'ë‚ ì”¨ ì˜ˆë³´', 'ì˜¤ëŠ˜ì˜ì¶”ì²œ': 'ë‰´ìŠ¤'\n",
        "        }\n",
        "\n",
        "    def get_naver_news(self, query, count=5):\n",
        "        \"\"\"ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•´ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
        "        url = \"https://openapi.naver.com/v1/search/news.json\"\n",
        "        headers = {'X-Naver-Client-Id': self.naver_client_id, 'X-Naver-Client-Secret': self.naver_client_secret}\n",
        "        params = {'query': query, 'display': count, 'start': 1, 'sort': 'date'}\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers, params=params, timeout=6)\n",
        "            r.raise_for_status()\n",
        "            return r.json().get('items', [])\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "    def get_newsapi_news(self, category, count=5):\n",
        "        \"\"\"NewsAPIë¥¼ í†µí•´ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
        "        url = \"https://newsapi.org/v2/top-headlines\"\n",
        "        params = {'apiKey': self.newsapi_key, 'pageSize': count, 'page': 1, 'category': map_category_newsapi(category), 'language': 'ko'}\n",
        "        try:\n",
        "            r = requests.get(url, params=params, timeout=6)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            return data.get('articles', [])\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "news_service = NewsService()\n",
        "\n",
        "def map_category_newsapi(category):\n",
        "    \"\"\"í•œêµ­ì–´ ì¹´í…Œê³ ë¦¬ë¥¼ NewsAPI ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.\"\"\"\n",
        "    # âš¡ï¸ NewsAPI ë§¤í•‘ ì—…ë°ì´íŠ¸: 'ì„¸ê³„', 'ë‚ ì”¨' ìœ ì§€\n",
        "    mapping = {\n",
        "        'ì •ì¹˜': None, 'ê²½ì œ': 'business', 'ì‚¬íšŒ': None, 'ìƒí™œë¬¸í™”': None, 'ì—°ì˜ˆ': 'entertainment', 'ìŠ¤í¬ì¸ ': 'sports',\n",
        "        'ITê³¼í•™': 'technology', 'ì„¸ê³„': 'general', 'ë‚ ì”¨': None\n",
        "    }\n",
        "    return mapping.get(category, None)\n",
        "\n",
        "# --- ì‹ ê·œ ìœ íš¨ì„± ê²€ì‚¬ í•¨ìˆ˜ ---\n",
        "def validate_text_relevance(title, full_text):\n",
        "    \"\"\"\n",
        "    ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì˜ ê´€ë ¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.\n",
        "    ì œëª©ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— ì¶©ë¶„íˆ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì˜ëª»ëœ í¬ë¡¤ë§ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    if not full_text or not title:\n",
        "        return False\n",
        "\n",
        "    # 1. ì œëª© í´ë¦¬ë‹: ê´„í˜¸, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "    clean_title = re.sub(r'\\[.*?\\]|\\(.*?\\)|[^\\w\\s]', ' ', title).strip()\n",
        "\n",
        "    # 2. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ: 2ê¸€ì ì´ìƒ ë‹¨ì–´ ì¤‘ ìƒìœ„ 3ê°œ ì‚¬ìš©\n",
        "    keywords = [w for w in clean_title.split() if len(w) > 1][:3]\n",
        "\n",
        "    if not keywords: return True # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ëŠ” ê²½ìš° í†µê³¼ (ì•ˆì „ëª¨ë“œ)\n",
        "\n",
        "    # 3. í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸\n",
        "    match_count = sum(1 for kw in keywords if kw in full_text)\n",
        "\n",
        "    # í‚¤ì›Œë“œì˜ ì ˆë°˜ ì´ìƒ(ìµœì†Œ 1ê°œ)ì´ ë³¸ë¬¸ì— í¬í•¨ë˜ì–´ì•¼ ìœ íš¨í•˜ë‹¤ê³  íŒë‹¨\n",
        "    required_matches = max(1, len(keywords) // 2)\n",
        "\n",
        "    # 4. í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ì‹¤íŒ¨\n",
        "    return match_count >= required_matches\n",
        "# ---\n",
        "\n",
        "# -----------------------------\n",
        "# ë°ì´í„° ì •ê·œí™” ë° ë³´ê°•\n",
        "# -----------------------------\n",
        "def normalize_and_enrich(items, source='naver', category='ì¼ë°˜'):\n",
        "    \"\"\"ë‰´ìŠ¤ í•­ëª©ì„ ì •ê·œí™”í•˜ê³  ìš”ì•½, ë³¸ë¬¸, ë©”íƒ€ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    item = items[0]\n",
        "\n",
        "    title = clean_text(item.get('title') or item.get('Title') or '')\n",
        "    description = clean_text(item.get('description') or item.get('content') or '')\n",
        "    link = item.get('link') or item.get('url')\n",
        "    full_text = None\n",
        "\n",
        "    # pub_date ì¶”ì¶œ ë¡œì§ (ì´ì „ê³¼ ë™ì¼)\n",
        "    pub_date_raw = item.get('pubDate') or item.get('publishedAt')\n",
        "    pub_date = pub_date_raw\n",
        "    if pub_date_raw:\n",
        "        try:\n",
        "            if source == 'naver':\n",
        "                dt_obj = datetime.strptime(pub_date_raw, '%a, %d %b %Y %H:%M:%S %z')\n",
        "            else: # NewsAPI\n",
        "                dt_obj = datetime.fromisoformat(pub_date_raw.replace('Z', '+00:00'))\n",
        "            pub_date = dt_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        # âš¡ï¸ Fix 1 (Part 1): api_similar_newsì—ì„œ title/descriptionì´ ë¹„ì–´ ë„˜ì–´ì™”ì„ ë•Œ ì±„ì›Œ ë„£ê¸°\n",
        "        # api_similar_newsëŠ” title/descriptionì´ ë¹„ì–´ìˆëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë³´ëƒ…ë‹ˆë‹¤. ì´ë¥¼ ì±„ì›Œë„£ì–´ fallback í…ìŠ¤íŠ¸ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.\n",
        "        if link and (not title or not description):\n",
        "            try:\n",
        "                # Trafilaturaë‚˜ Newspaper3kë¥¼ ì¬ì‹œë„í•˜ê¸° ì „ì— Articleì„ ì´ìš©í•´ ì œëª©/ì„¤ëª…ì„ ê°€ì ¸ì™€ fallback í…ìŠ¤íŠ¸ë¥¼ í’ë¶€í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
        "                temp_article = Article(link, language='ko')\n",
        "                temp_article.download()\n",
        "                temp_article.parse()\n",
        "                if temp_article.title and len(clean_text(temp_article.title)) > len(title):\n",
        "                    title = clean_text(temp_article.title)\n",
        "                if temp_article.meta_description and len(clean_text(temp_article.meta_description)) > len(description):\n",
        "                    description = clean_text(temp_article.meta_description)\n",
        "            except Exception:\n",
        "                pass # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ì§„í–‰\n",
        "\n",
        "        # 1. ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (Trafilatura ìš°ì„ )\n",
        "        full_text = get_article_text(link)\n",
        "        text_to_summarize = full_text\n",
        "\n",
        "        # 2. ğŸš¨ ìœ íš¨ì„± ê²€ì‚¬ ë° í…ìŠ¤íŠ¸ ê²°ì • ë¡œì§\n",
        "        is_text_valid = validate_text_relevance(title, full_text)\n",
        "\n",
        "        if not full_text or len(full_text) < 100 or not is_text_valid:\n",
        "            # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ê°€ë¹„ì§€ í…ìŠ¤íŠ¸ë¡œ íŒë‹¨ëœ ê²½ìš°: ì•ˆì „í•œ í´ë°± í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
        "            if full_text and not is_text_valid:\n",
        "                 print(f\"âš ï¸ í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì´ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. ì•ˆì „ëª¨ë“œ ì§„ì….\")\n",
        "            elif not full_text:\n",
        "                 print(f\"âš ï¸ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: full_textê°€ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „ëª¨ë“œ ì§„ì….\")\n",
        "\n",
        "            # âš¡ï¸ Fix 1 (Part 2): ì•ˆì „í•œ titleê³¼ descriptionì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ì‹œë„\n",
        "            text_to_summarize = title + \". \" + description\n",
        "            if len(text_to_summarize) < 50:\n",
        "                 # ì •ë§ ì§§ì€ ê²½ìš°, ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ë¥¼ ì œê³µ\n",
        "                 text_to_summarize = title or description or \"ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒì„¸ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        # 3. ìš”ì•½ ìƒì„± (ëª¨ë¸ ë˜ëŠ” Fallback)\n",
        "        detailed_summary = make_summary(text_to_summarize)\n",
        "\n",
        "        # 4. ë¯¸ë¦¬ë³´ê¸° ìš”ì•½ ìƒì„± (ì²« ë‘ ë¬¸ì¥)\n",
        "        sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', detailed_summary)\n",
        "        preview_summary = \" \".join(sentences[:2])\n",
        "        if len(preview_summary) > 50:\n",
        "            preview_summary = preview_summary[:50].strip() + \"...\"\n",
        "        elif not preview_summary.endswith(('.', '!', '?', 'â€¦')):\n",
        "            preview_summary += \"...\"\n",
        "\n",
        "        publisher_name = extract_publisher_name(link)\n",
        "        image_url, views = extract_image_and_views(link)\n",
        "        views_display = str(views) if views is not None else \"None\"\n",
        "\n",
        "        news_item = {\n",
        "            \"title\": title, \"description\": description, \"link\": link, \"pub_date\": pub_date,\n",
        "            \"image_url\": image_url, \"view_count\": views_display,\n",
        "            \"preview_summary\": preview_summary, \"detailed_summary\": detailed_summary,\n",
        "            \"source\": source, \"category\": category,\n",
        "            \"publisher\": publisher_name,\n",
        "            \"full_text\": full_text # ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ ë³¸ë¬¸ í¬í•¨\n",
        "        }\n",
        "        return news_item\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# --- ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë¡œì§ ---\n",
        "def find_similar_news(main_article_content, main_link, category, count=3):\n",
        "    \"\"\"ì£¼ìš” ê¸°ì‚¬ ë‚´ìš©(main_article_content)ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤. (í›„ë³´êµ° 10ê°œ ë‚´ì™¸)\"\"\"\n",
        "    start_time = time.time()\n",
        "    print(f\"ğŸ” ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘ (ì›ë³¸ ê¸°ì‚¬: {main_link[:50]}...)\")\n",
        "\n",
        "    # ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ë‰´ìŠ¤ í›„ë³´êµ° ê°€ì ¸ì˜¤ê¸°:\n",
        "    search_queries = [news_service.query_map.get(category, category)] # ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì¿¼ë¦¬ë§Œ ì‚¬ìš©\n",
        "    raw_news_candidates = []\n",
        "\n",
        "    # 1. ë„¤ì´ë²„ì—ì„œ 5ê°œ ê°€ì ¸ì˜¤ê¸° (ì¹´í…Œê³ ë¦¬ ì¿¼ë¦¬)\n",
        "    for query in search_queries:\n",
        "        naver_items = news_service.get_naver_news(query, count=5)\n",
        "        raw_news_candidates.extend(naver_items)\n",
        "\n",
        "    # 2. NewsAPIì—ì„œ 5ê°œ ê°€ì ¸ì˜¤ê¸° (ì¹´í…Œê³ ë¦¬ ì¿¼ë¦¬)\n",
        "    newsapi_items = news_service.get_newsapi_news(category, count=5)\n",
        "    raw_news_candidates.extend(newsapi_items)\n",
        "\n",
        "    # ì¤‘ë³µ ë§í¬ ì œê±° ë° ë³¸ì¸ ê¸°ì‚¬ ì œì™¸\n",
        "    unique_candidates = {}\n",
        "    for item in raw_news_candidates:\n",
        "        link = item.get('link') or item.get('url')\n",
        "        if link and link != main_link and link not in unique_candidates:\n",
        "            unique_candidates[link] = item\n",
        "\n",
        "    all_raw_items = list(unique_candidates.values())\n",
        "\n",
        "    processed_candidates = []\n",
        "    # í›„ë³´ ê¸°ì‚¬ ë³‘ë ¬ ì²˜ë¦¬ (ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½)\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: # max_workersë„ 5ê°œë¡œ ì¤„ì„\n",
        "        futures = {executor.submit(normalize_and_enrich, [item], 'naver' if item.get('link') else 'newsapi', category) for item in all_raw_items}\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            processed_item = future.result()\n",
        "            if processed_item and processed_item.get('full_text'):\n",
        "                processed_candidates.append(processed_item)\n",
        "\n",
        "    if not processed_candidates:\n",
        "        print(\"âš ï¸ ìœ ì‚¬ ë‰´ìŠ¤ í›„ë³´êµ°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return []\n",
        "\n",
        "    # TF-IDF ë²¡í„°í™” ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    corpus = [main_article_content] + [item['full_text'] for item in processed_candidates]\n",
        "    # TfidfVectorizerëŠ” í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    vectorizer = TfidfVectorizer().fit_transform(corpus)\n",
        "\n",
        "    cosine_similarities = cosine_similarity(vectorizer[0:1], vectorizer[1:]).flatten()\n",
        "\n",
        "    similar_indices = np.argsort(cosine_similarities)[::-1]\n",
        "\n",
        "    similar_news = []\n",
        "    found_count = 0\n",
        "    for i in similar_indices:\n",
        "        if cosine_similarities[i] > 0.3: # ìœ ì‚¬ë„ 0.3 ì´ìƒë§Œ ìœ íš¨í•˜ë‹¤ê³  ê°„ì£¼\n",
        "            processed_candidates[i]['similarity_score'] = f\"{cosine_similarities[i]:.2f}\"\n",
        "            similar_news.append(processed_candidates[i])\n",
        "            found_count += 1\n",
        "            if found_count >= count:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"â±ï¸ ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ (ìµœì¢… í›„ë³´ ìˆ˜: {len(processed_candidates)})\")\n",
        "\n",
        "    return similar_news\n",
        "# ---\n",
        "\n",
        "# -----------------------------\n",
        "# Flask API ì—”ë“œí¬ì¸íŠ¸\n",
        "# -----------------------------\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    \"\"\"CORS í—¤ë”ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
        "    response.headers.add('Access-Control-Allow-Methods', 'GET,POST,OPTIONS')\n",
        "    return response\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    \"\"\"ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸.\"\"\"\n",
        "    return jsonify({\"message\": \"ğŸ—ï¸ ë‰´ìŠ¤ ì¶”ì²œ AI ì„œë²„ (í´ë¦­ ì‹œ ë¡œë”© ë²„ì „, ì•ˆì •í™”) ì‹¤í–‰ì¤‘!\", \"status\": \"running\"})\n",
        "\n",
        "\n",
        "@app.route('/api/news', defaults={'category': None})\n",
        "@app.route('/api/news/<category>')\n",
        "def api_news(category):\n",
        "    \"\"\"\n",
        "    ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê³  ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. (ë©”ì¸ ëª©ë¡ìš© - ë¹ ë¦„)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    selected_cats = request.args.get('categories')\n",
        "    selected_list = selected_cats.split(',') if selected_cats else []\n",
        "    # í•„í„°ë§ ì¸ì ì²˜ë¦¬ ('', 'None' ë“±ì´ ë„˜ì–´ì˜¬ ìˆ˜ ìˆìŒ)\n",
        "    include_publishers = request.args.get('include_publishers', '').split(',')\n",
        "    exclude_publishers = request.args.get('exclude_publishers', '').split(',')\n",
        "\n",
        "    all_raw_items = []\n",
        "    categories_to_fetch = [category] if category and category != 'ì˜¤ëŠ˜ì˜ì¶”ì²œ' else (selected_list or list(CATEGORIES.keys()))\n",
        "\n",
        "    for cat in categories_to_fetch:\n",
        "        # âš¡ï¸ NewsService.query_map ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½\n",
        "        query = news_service.query_map.get(cat, cat)\n",
        "        naver_items = news_service.get_naver_news(query, count=5)\n",
        "        all_raw_items.extend([(item, 'naver', cat) for item in naver_items])\n",
        "\n",
        "    for cat in categories_to_fetch:\n",
        "        newsapi_items = news_service.get_newsapi_news(cat, count=5)\n",
        "        all_raw_items.extend([(item, 'newsapi', cat) for item in newsapi_items])\n",
        "\n",
        "    result = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = {executor.submit(normalize_and_enrich, [raw_item], source, cat) for raw_item, source, cat in all_raw_items}\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            processed_item = future.result()\n",
        "            if processed_item:\n",
        "                # ë©”ì¸ ëª©ë¡ì—ì„œëŠ” 'full_text'ë¥¼ ì œê±°í•˜ì—¬ ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.\n",
        "                if 'full_text' in processed_item:\n",
        "                    del processed_item['full_text']\n",
        "                result.append(processed_item)\n",
        "\n",
        "    filtered_result = []\n",
        "    # í•„í„°ë§ ë¡œì§ ê°œì„ : ë¹ˆ ë¬¸ìì—´ í•„í„° ì²˜ë¦¬\n",
        "    include_set = set(p.strip() for p in include_publishers if p.strip())\n",
        "    exclude_set = set(p.strip() for p in exclude_publishers if p.strip())\n",
        "\n",
        "    for item in result:\n",
        "        publisher = item.get('publisher')\n",
        "        # í¬í•¨ í•„í„° ì ìš©\n",
        "        if include_set and publisher and publisher not in include_set:\n",
        "            continue\n",
        "        # ì œì™¸ í•„í„° ì ìš©\n",
        "        if exclude_set and publisher and publisher in exclude_set:\n",
        "            continue\n",
        "        filtered_result.append(item)\n",
        "\n",
        "    # ì¤‘ë³µ ì œê±° (link ê¸°ì¤€ìœ¼ë¡œ)\n",
        "    unique_links = set()\n",
        "    final_result = []\n",
        "    for item in filtered_result:\n",
        "        link = item.get('link')\n",
        "        if link not in unique_links:\n",
        "            unique_links.add(link)\n",
        "            final_result.append(item)\n",
        "\n",
        "    final_result = sorted(final_result, key=lambda x: x.get('pub_date') or '', reverse=True)[:5]\n",
        "    end_time = time.time()\n",
        "    print(f\"â±ï¸ ë©”ì¸ ë‰´ìŠ¤ API ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
        "\n",
        "    return jsonify(final_result)\n",
        "\n",
        "\n",
        "@app.route('/api/similar_news')\n",
        "def api_similar_news():\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ê¸°ì‚¬ì˜ URLê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ë°›ì•„ ìœ ì‚¬í•œ ë‰´ìŠ¤ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤. (í´ë¦­ ì‹œ í˜¸ì¶œ)\n",
        "    \"\"\"\n",
        "    main_link = request.args.get('link')\n",
        "    category = request.args.get('category', 'all')\n",
        "    if not main_link:\n",
        "        return jsonify({\"error\": \"ê¸°ì‚¬ ë§í¬(link)ê°€ í•„ìš”í•©ë‹ˆë‹¤.\"}), 400\n",
        "\n",
        "    # 1. ë©”ì¸ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ì‹œê°„ì´ ê±¸ë¦¼)\n",
        "    main_article = normalize_and_enrich([{'link': main_link, 'title': '', 'description': '', 'pubDate': ''}], source='naver', category=category)\n",
        "\n",
        "    if not main_article:\n",
        "        return jsonify({\n",
        "            \"detailed_summary\": \"ê¸°ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\",\n",
        "            \"similar_news\": []\n",
        "        }), 500\n",
        "\n",
        "    if not main_article.get('full_text') or len(main_article['full_text']) < 100 or not validate_text_relevance(main_article['title'], main_article['full_text']):\n",
        "        # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ë‚˜ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ ì‹œ ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë¶ˆê°€\n",
        "        print(\"âš ï¸ ì›ë³¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë˜ëŠ” ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨. ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ë¶ˆê°€.\")\n",
        "        # detailed_summaryëŠ” ì´ë¯¸ ì•ˆì „í•œ title+description ê¸°ë°˜ì˜ ìš”ì•½ì¼ ê²ƒì…ë‹ˆë‹¤.\n",
        "        return jsonify({\n",
        "            \"detailed_summary\": main_article.get('detailed_summary', \"ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒì„¸ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\"),\n",
        "            \"similar_news\": []\n",
        "        })\n",
        "\n",
        "    # 2. ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ (TF-IDF ê¸°ë°˜)\n",
        "    similar_news_list = find_similar_news(main_article['full_text'], main_link, category)\n",
        "\n",
        "    # 3. ì‘ë‹µì— í¬í•¨ë  ìœ ì‚¬ ë‰´ìŠ¤ì—ì„œ 'full_text' ì œê±°\n",
        "    for item in similar_news_list:\n",
        "        if 'full_text' in item:\n",
        "            del item['full_text']\n",
        "\n",
        "    # 4. ìƒì„¸ ìš”ì•½ê³¼ ìœ ì‚¬ ë‰´ìŠ¤ë¥¼ í•¨ê»˜ ë°˜í™˜\n",
        "    return jsonify({\n",
        "        \"detailed_summary\": main_article['detailed_summary'],\n",
        "        \"similar_news\": similar_news_list\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Flask TEST Page (ë²„íŠ¼ í´ë¦­ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ)\n",
        "# -----------------------------\n",
        "\n",
        "@app.route('/test')\n",
        "def test_page():\n",
        "    \"\"\"ì›¹ í…ŒìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.\"\"\"\n",
        "    # JavaScript ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë¥¼ DOMContentLoaded ë‚´ë¶€ì— ë°°ì¹˜í•˜ì—¬ ì•ˆì •ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.\n",
        "    TEST_PAGE_HTML = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "<html lang=\"ko\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "<title>ë‰´ìŠ¤ AI ì„œë²„ í…ŒìŠ¤íŠ¸</title>\n",
        "<style>\n",
        "body{font-family:Segoe UI,Roboto,Arial;background:linear-gradient(135deg,#667eea,#764ba2);margin:0;padding:20px}\n",
        ".container{max-width:1100px;margin:0 auto;background:#fff;border-radius:10px;overflow:hidden;box-shadow:0 10px 30px rgba(0,0,0,.15)}\n",
        ".header{background:#2c3e50;color:#fff;padding:18px;text-align:center}\n",
        ".controls{padding:16px;background:#f8f9fa;border-bottom:1px solid #e9ecef}\n",
        ".input-group{display:flex;gap:10px;align-items:center;flex-wrap:wrap}\n",
        "select,input{padding:8px;border-radius:6px;border:1px solid #ddd}\n",
        ".btn{padding:4px 8px;border-radius:6px;color:#fff;border:none;cursor:pointer;transition:background-color .2s ease; margin: 2px 2px;}\n",
        ".btn-primary{background:#007bff}\n",
        ".btn-success{background:#2ecc71}\n",
        ".btn-danger{background:#e74c3c}\n",
        ".results{padding:16px}\n",
        ".news-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(320px,1fr));gap:16px}\n",
        ".news-card{border:1px solid #eee;border-radius:8px;overflow:hidden;background:#fff;position:relative; cursor:pointer; transition: box-shadow 0.3s;}\n",
        ".news-card:hover{box-shadow: 0 4px 15px rgba(0,0,0,.1);}\n",
        ".news-card.active { box-shadow: 0 0 15px rgba(0, 123, 255, 0.5); border-color: #007bff; }\n",
        ".news-card img{width:100%;height:180px;object-fit:cover}\n",
        ".content{padding:12px}\n",
        ".meta{font-size:12px;color:#666;margin-top:8px}\n",
        ".filter-section h4{margin-top:0; margin-bottom: 5px;}\n",
        ".similar-news-section{margin-top:15px;padding-top:10px;border-top:1px dashed #ccc;}\n",
        ".similar-news-section h4{margin-top:0; color:#2c3e50;}\n",
        ".loading-spinner {border: 4px solid #f3f3f3; border-top: 4px solid #3498db; border-radius: 50%; width: 16px; height: 16px; animation: spin 2s linear infinite; display: inline-block; margin-right: 5px;}\n",
        "@keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }\n",
        ".similar-news-grid {display: grid; grid-template-columns: repeat(auto-fill, minmax(280px, 1fr)); gap: 10px; margin-top: 10px;}\n",
        ".similar-news-card {border: 1px solid #2ecc71; border-radius: 6px; padding: 10px; background: #f0fff7; font-size: 14px;}\n",
        ".similar-news-card h5 {margin-top: 0; margin-bottom: 5px; font-size: 15px; color: #2ecc71;}\n",
        ".similar-news-card p {margin: 0; font-size: 13px;}\n",
        "\n",
        "/* ì¶”ê°€ëœ CSS */\n",
        ".filter-section .btn {\n",
        "    margin: 2px; /* ë²„íŠ¼ ê°„ê²© ì¡°ì • */\n",
        "}\n",
        "\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\"><h2>ë‰´ìŠ¤ AI ì„œë²„ í…ŒìŠ¤íŠ¸ </h2><div>ì„œë²„ ì‹¤í–‰ì¤‘</div></div>\n",
        "    <div class=\"controls\">\n",
        "        <div style=\"margin-bottom:4px;\">\n",
        "            <label>ì¹´í…Œê³ ë¦¬:</label>\n",
        "            <select id=\"category\">\n",
        "                <option value=\"ì •ì¹˜\">ì •ì¹˜</option>\n",
        "                <option value=\"ê²½ì œ\">ê²½ì œ</option>\n",
        "                <option value=\"ì‚¬íšŒ\">ì‚¬íšŒ</option>\n",
        "                <option value=\"ìƒí™œë¬¸í™”\">ìƒí™œë¬¸í™”</option>\n",
        "                <option value=\"ì—°ì˜ˆ\">ì—°ì˜ˆ</option>\n",
        "                <option value=\"ìŠ¤í¬ì¸ \">ìŠ¤í¬ì¸ </option>\n",
        "                <option value=\"ì„¸ê³„\">ì„¸ê³„</option> <!-- âš¡ï¸ 'ê±´ê°•' -> 'ì„¸ê³„' -->\n",
        "                <option value=\"ë‚ ì”¨\">ë‚ ì”¨</option> <!-- âš¡ï¸ 'ë‚ ì”¨' ì¶”ê°€ -->\n",
        "                <option value=\"ITê³¼í•™\">ITê³¼í•™</option>\n",
        "                <option value=\"ì˜¤ëŠ˜ì˜ì¶”ì²œ\">ì˜¤ëŠ˜ì˜ì¶”ì²œ</option>\n",
        "            </select>\n",
        "            <button class=\"btn btn-primary\" onclick=\"fetchNews()\">ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° </button>\n",
        "        </div>\n",
        "        <div id=\"todayBtns\" style=\"margin-bottom:2px;\">\n",
        "            <label>ì˜¤ëŠ˜ì˜ì¶”ì²œ ì„ íƒ:</label>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì •ì¹˜\">ì •ì¹˜</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ê²½ì œ\">ê²½ì œ</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì‚¬íšŒ\">ì‚¬íšŒ</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ìƒí™œë¬¸í™”\">ìƒí™œë¬¸í™”</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì—°ì˜ˆ\">ì—°ì˜ˆ</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ìŠ¤í¬ì¸ \">ìŠ¤í¬ì¸ </button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì„¸ê³„\">ì„¸ê³„</button> <!-- âš¡ï¸ 'ê±´ê°•' -> 'ì„¸ê³„' -->\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ë‚ ì”¨\">ë‚ ì”¨</button> <!-- âš¡ï¸ 'ë‚ ì”¨' ì¶”ê°€ -->\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ITê³¼í•™\">ITê³¼í•™</button>\n",
        "        </div>\n",
        "        <div class=\"filter-section\">\n",
        "            <h4>ì–¸ë¡ ì‚¬ í¬í•¨ (í´ë¦­í•˜ë©´ ì„ íƒ)</h4>\n",
        "            <div id=\"include-publishers-btns\">\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"ì—°í•©ë‰´ìŠ¤\">ì—°í•©ë‰´ìŠ¤</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"ì¡°ì„ ì¼ë³´\">ì¡°ì„ ì¼ë³´</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"ì¤‘ì•™ì¼ë³´\">ì¤‘ì•™ì¼ë³´</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"ë™ì•„ì¼ë³´\">ë™ì•„ì¼ë³´</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"í•œê²¨ë ˆ\">í•œê²¨ë ˆ</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"ë§¤ì¼ê²½ì œ\">ë§¤ì¼ê²½ì œ</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"íŒŒì´ë‚¸ì…œë‰´ìŠ¤\">íŒŒì´ë‚¸ì…œë‰´ìŠ¤</button>\n",
        "                <button class=\"btn btn-primary btn-include\" data-publisher=\"ë‰´ìŠ¤1\">ë‰´ìŠ¤1</button>\n",
        "            </div>\n",
        "            <div id=\"selected-includes\" style=\"margin-top:5px; font-size:14px;\">ì„ íƒë¨: ì—†ìŒ</div>\n",
        "        </div>\n",
        "        <div class=\"filter-section\">\n",
        "            <h4>ì–¸ë¡ ì‚¬ ì œì™¸ (í´ë¦­í•˜ë©´ ì„ íƒ)</h4>\n",
        "            <div id=\"exclude-publishers-btns\">\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"ì—°í•©ë‰´ìŠ¤\">ì—°í•©ë‰´ìŠ¤</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"ì¡°ì„ ì¼ë³´\">ì¡°ì„ ì¼ë³´</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"ì¤‘ì•™ì¼ë³´\">ì¤‘ì•™ì¼ë³´</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"ë™ì•„ì¼ë³´\">ë™ì•„ì¼ë³´</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"í•œê²¨ë ˆ\">í•œê²¨ë ˆ</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"ë§¤ì¼ê²½ì œ\">ë§¤ì¼ê²½ì œ</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"íŒŒì´ë‚¸ì…œë‰´ìŠ¤\">íŒŒì´ë‚¸ì…œë‰´ìŠ¤</button>\n",
        "                <button class=\"btn btn-primary btn-exclude\" data-publisher=\"ë‰´ìŠ¤1\">ë‰´ìŠ¤1</button>\n",
        "            </div>\n",
        "            <div id=\"selected-excludes\" style=\"margin-top:5px; font-size:14px;\">ì„ íƒë¨: ì—†ìŒ</div>\n",
        "        </div>\n",
        "    </div>\n",
        "    <div class=\"results\">\n",
        "        <div id=\"status\"></div>\n",
        "        <div id=\"newsContainer\"></div>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "let newsData = null;\n",
        "let selectedTodayCategories = [];\n",
        "let includePublishers = new Set();\n",
        "let excludePublishers = new Set();\n",
        "let currentActiveCard = null;\n",
        "\n",
        "function updateSelectedDisplay(containerId, dataSet) {\n",
        "    const display = document.getElementById(containerId);\n",
        "    display.innerText = \"ì„ íƒë¨: \" + (dataSet.size > 0 ? Array.from(dataSet).join(', ') : 'ì—†ìŒ');\n",
        "}\n",
        "\n",
        "// *** í•µì‹¬ ìˆ˜ì •: ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë¥¼ DOMContentLoaded ë‚´ì— ë°°ì¹˜í•˜ì—¬ ì•ˆì •ì„± í™•ë³´ ***\n",
        "window.addEventListener('DOMContentLoaded', () => {\n",
        "\n",
        "    // ì˜¤ëŠ˜ì˜ì¶”ì²œ ë²„íŠ¼ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ\n",
        "    document.querySelectorAll('.today-btn').forEach(btn => {\n",
        "        btn.addEventListener('click', () => {\n",
        "            const cat = btn.dataset.cat;\n",
        "            if (selectedTodayCategories.includes(cat)) {\n",
        "                selectedTodayCategories = selectedTodayCategories.filter(c => c !== cat);\n",
        "                btn.classList.remove('btn-success');\n",
        "                btn.classList.add('btn-primary');\n",
        "            } else {\n",
        "                // ì´ì „ì— 'ê±´ê°•'ì´ì—ˆë˜ ë²„íŠ¼ë“¤ì€ 'ì„¸ê³„', 'ë‚ ì”¨'ë¡œ ëŒ€ì²´ë˜ì—ˆì§€ë§Œ data-cat ì†ì„±ì€ ì •í™•í•©ë‹ˆë‹¤.\n",
        "                selectedTodayCategories.push(cat);\n",
        "                btn.classList.add('btn-success');\n",
        "                btn.classList.remove('btn-primary');\n",
        "            }\n",
        "        });\n",
        "    });\n",
        "\n",
        "    // ì–¸ë¡ ì‚¬ í¬í•¨ ë²„íŠ¼ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ\n",
        "    document.querySelectorAll('.btn-include').forEach(btn => {\n",
        "        btn.addEventListener('click', () => {\n",
        "            const publisher = btn.dataset.publisher;\n",
        "            if (includePublishers.has(publisher)) {\n",
        "                includePublishers.delete(publisher);\n",
        "                btn.classList.remove('btn-success');\n",
        "                btn.classList.add('btn-primary');\n",
        "            } else {\n",
        "                if (excludePublishers.has(publisher)) {\n",
        "                    console.warn('í¬í•¨/ì œì™¸ í•„í„°ëŠ” ì¤‘ë³µ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');\n",
        "                    return;\n",
        "                }\n",
        "                includePublishers.add(publisher);\n",
        "                btn.classList.add('btn-success');\n",
        "                btn.classList.remove('btn-primary');\n",
        "            }\n",
        "            updateSelectedDisplay('selected-includes', includePublishers);\n",
        "        });\n",
        "    });\n",
        "\n",
        "    // ì–¸ë¡ ì‚¬ ì œì™¸ ë²„íŠ¼ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ\n",
        "    document.querySelectorAll('.btn-exclude').forEach(btn => {\n",
        "        btn.addEventListener('click', () => {\n",
        "            const publisher = btn.dataset.publisher;\n",
        "            if (excludePublishers.has(publisher)) {\n",
        "                excludePublishers.delete(publisher);\n",
        "                btn.classList.remove('btn-danger');\n",
        "                btn.classList.add('btn-primary');\n",
        "            } else {\n",
        "                if (includePublishers.has(publisher)) {\n",
        "                    console.warn('í¬í•¨/ì œì™¸ í•„í„°ëŠ” ì¤‘ë³µ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');\n",
        "                    return;\n",
        "                }\n",
        "                excludePublishers.add(publisher);\n",
        "                btn.classList.add('btn-danger');\n",
        "                btn.classList.remove('btn-primary');\n",
        "            }\n",
        "            updateSelectedDisplay('selected-excludes', excludePublishers);\n",
        "        });\n",
        "    });\n",
        "});\n",
        "// ------------------------------------\n",
        "\n",
        "function setStatus(msg, type='') {\n",
        "    const s = document.getElementById('status');\n",
        "    s.innerHTML = msg;\n",
        "    s.style.color = type==='error' ? '#c0392b' : '#2ecc71';\n",
        "}\n",
        "\n",
        "// --- 1. ë©”ì¸ ë‰´ìŠ¤ ëª©ë¡ ë¡œë“œ (ë¹ ë¥¸ ë¡œë”©) ---\n",
        "async function fetchNews() {\n",
        "    const startTime = new Date();\n",
        "    const cat = document.getElementById('category').value;\n",
        "\n",
        "    let endpoint = '/api/news';\n",
        "    const params = new URLSearchParams();\n",
        "\n",
        "    // í•„í„°ë§ íŒŒë¼ë¯¸í„° êµ¬ì„±\n",
        "    if (cat) { endpoint += '/' + encodeURIComponent(cat); }\n",
        "    if (selectedTodayCategories.length) { params.append('categories', selectedTodayCategories.join(',')); }\n",
        "    if (includePublishers.size > 0) { params.append('include_publishers', Array.from(includePublishers).join(',')); }\n",
        "    if (excludePublishers.size > 0) { params.append('exclude_publishers', Array.from(excludePublishers).join(',')); }\n",
        "    if (params.toString()) { endpoint += '?' + params.toString(); }\n",
        "\n",
        "    setStatus('ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...');\n",
        "    currentActiveCard = null; // í™œì„± ì¹´ë“œ ì´ˆê¸°í™”\n",
        "\n",
        "    try {\n",
        "        const resp = await fetch(endpoint);\n",
        "        const data = await resp.json();\n",
        "        newsData = data;\n",
        "        renderNews(newsData);\n",
        "\n",
        "        const endTime = new Date();\n",
        "        const elapsedTime = (endTime - startTime) / 1000;\n",
        "        setStatus(`âœ… ëª©ë¡ ë¡œë“œ ì™„ë£Œ (${elapsedTime.toFixed(2)}ì´ˆ ê±¸ë¦¼). ê¸°ì‚¬ë¥¼ í´ë¦­í•˜ë©´ ìœ ì‚¬ ë‰´ìŠ¤ê°€ ë¡œë”©ë©ë‹ˆë‹¤.`);\n",
        "    } catch (e) {\n",
        "        setStatus('ìš”ì²­ ì‹¤íŒ¨: ' + e.message, 'error');\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- 2. ë‰´ìŠ¤ ì¹´ë“œ í´ë¦­ ì‹œ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ---\n",
        "async function fetchSimilarNews(link, category, cardElement) {\n",
        "\n",
        "    const similarSectionDiv = cardElement.querySelector('.similar-news-section');\n",
        "    const similarStatusDiv = cardElement.querySelector('.similar-status');\n",
        "\n",
        "    // 1. í† ê¸€ ê¸°ëŠ¥: ì´ë¯¸ ë¡œë“œë˜ì—ˆê³  í˜„ì¬ í™œì„± ìƒíƒœë¼ë©´ ë‹«ê¸°\n",
        "    if (currentActiveCard === cardElement) {\n",
        "        similarSectionDiv.style.display = 'none';\n",
        "        similarStatusDiv.innerText = '(í´ë¦­í•˜ì—¬ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ)';\n",
        "        cardElement.classList.remove('active');\n",
        "        currentActiveCard = null;\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // 2. ë‹¤ë¥¸ ì¹´ë“œê°€ í™œì„± ìƒíƒœì˜€ë‹¤ë©´ ë‹«ê¸°\n",
        "    if (currentActiveCard) {\n",
        "        currentActiveCard.querySelector('.similar-news-section').style.display = 'none';\n",
        "        currentActiveCard.querySelector('.similar-status').innerText = '(í´ë¦­í•˜ì—¬ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ)';\n",
        "        currentActiveCard.classList.remove('active');\n",
        "    }\n",
        "\n",
        "    // 3. ë¡œë”© ì‹œì‘ ë° ìƒíƒœ í‘œì‹œ\n",
        "    currentActiveCard = cardElement;\n",
        "    similarSectionDiv.innerHTML = '';\n",
        "    similarStatusDiv.innerHTML = '<span class=\"loading-spinner\"></span> ìœ ì‚¬ ë‰´ìŠ¤ ì°¾ëŠ” ì¤‘... (ì•½ 5~15ì´ˆ ì†Œìš”)';\n",
        "    similarStatusDiv.style.color = '#007bff';\n",
        "    cardElement.classList.add('active');\n",
        "\n",
        "    try {\n",
        "        const resp = await fetch(`/api/similar_news?link=${encodeURIComponent(link)}&category=${encodeURIComponent(category)}`);\n",
        "        const data = await resp.json();\n",
        "\n",
        "        if (data.error) {\n",
        "            similarStatusDiv.innerText = `ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: ${data.error}`;\n",
        "            similarStatusDiv.style.color = '#c0392b';\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // ìƒì„¸ ìš”ì•½ì„ ê¸°ì¡´ ì¹´ë“œì— ì—…ë°ì´íŠ¸ (APIì—ì„œ ìƒˆë¡œ ë°›ì•„ì˜¨ ìƒì„¸ ìš”ì•½ìœ¼ë¡œ êµì²´)\n",
        "        const detailedSummaryValueSpan = cardElement.querySelector('.detailed-summary-text-value');\n",
        "        const newSummary = data.detailed_summary;\n",
        "        // ì„œë²„ì—ì„œ ë°˜í™˜ë  ìˆ˜ ìˆëŠ” ì‹¤íŒ¨ ë©”ì‹œì§€ (Python ì½”ë“œ ì°¸ê³ )\n",
        "        const failedMessage1 = \"ê¸°ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\";\n",
        "        const failedMessage2 = \"ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒì„¸ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\";\n",
        "\n",
        "        // âš¡ï¸ Fix 1: ìƒˆë¡œ ë°›ì•„ì˜¨ ìš”ì•½ì´ ì‹¤íŒ¨ ë©”ì‹œì§€ì¸ ê²½ìš° ê¸°ì¡´ ìš”ì•½ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
        "        if (detailedSummaryValueSpan && !newSummary.includes(failedMessage1) && !newSummary.includes(failedMessage2)) {\n",
        "            detailedSummaryValueSpan.textContent = newSummary;\n",
        "        }\n",
        "        // ì‹¤íŒ¨ ë©”ì‹œì§€ì¸ ê²½ìš°: ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šì•„ ê¸°ì¡´ ìš”ì•½ì„ ìœ ì§€\n",
        "\n",
        "        // ìœ ì‚¬ ë‰´ìŠ¤ í‘œì‹œ\n",
        "        renderSimilarNews(data.similar_news, cardElement);\n",
        "        similarStatusDiv.innerText = 'ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ';\n",
        "        similarStatusDiv.style.color = '#2ecc71';\n",
        "\n",
        "    } catch (e) {\n",
        "        similarStatusDiv.innerText = 'ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ (ì„œë²„ ì—°ê²° ì˜¤ë¥˜)';\n",
        "        similarStatusDiv.style.color = '#c0392b';\n",
        "        console.error('ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨:', e);\n",
        "    }\n",
        "}\n",
        "\n",
        "function renderNews(data) {\n",
        "    const container = document.getElementById('newsContainer');\n",
        "    container.innerHTML = '';\n",
        "    if (!Array.isArray(data) || !data.length) {\n",
        "        container.innerText = 'ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.';\n",
        "        return;\n",
        "    }\n",
        "    const grid = document.createElement('div');\n",
        "    grid.className = 'news-grid';\n",
        "\n",
        "    data.forEach(n => {\n",
        "        const card = document.createElement('div');\n",
        "        card.className = 'news-card';\n",
        "\n",
        "        // í´ë¦­ ì´ë²¤íŠ¸ ì„¤ì •\n",
        "        card.onclick = () => fetchSimilarNews(n.link, n.category, card);\n",
        "\n",
        "        const img = document.createElement('img');\n",
        "        img.src = n.image_url || 'https://via.placeholder.com/400x200?text=No+Image';\n",
        "        card.appendChild(img);\n",
        "\n",
        "        const content = document.createElement('div');\n",
        "        content.className = 'content';\n",
        "\n",
        "        // âš¡ï¸ Fix 1: ìƒì„¸ë³´ê¸° í…ìŠ¤íŠ¸ë¥¼ <span>ìœ¼ë¡œ ê°ì‹¸ì„œ JavaScriptì—ì„œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ê²Œ í•¨\n",
        "        content.innerHTML = `\n",
        "            <h3>${n.title}</h3>\n",
        "            <p><b>ë¯¸ë¦¬ë³´ê¸°:</b> ${n.preview_summary}</p>\n",
        "            <p class=\"detailed-summary-container\"><b>ìƒì„¸ë³´ê¸°:</b> <span class=\"detailed-summary-text-value\">${n.detailed_summary}</span></p>\n",
        "            <div class=\"meta\">${n.pub_date || ''} | ${n.publisher || ''} | ${n.category || 'ê¸°íƒ€'}</div>\n",
        "            <a href=\"${n.link}\" target=\"_blank\" onclick=\"event.stopPropagation();\">ì›ë¬¸ ë³´ê¸°</a>\n",
        "            <div class=\"similar-status\" style=\"font-size:12px;margin-top:5px; height: 16px;\">(í´ë¦­í•˜ì—¬ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ)</div>\n",
        "            <div class=\"similar-news-section\" style=\"display:none;\"></div>\n",
        "        `;\n",
        "        // **********************************************\n",
        "        card.appendChild(content);\n",
        "        grid.appendChild(card);\n",
        "    });\n",
        "    container.appendChild(grid);\n",
        "}\n",
        "\n",
        "function renderSimilarNews(data, cardElement) {\n",
        "    const similarSection = cardElement.querySelector('.similar-news-section');\n",
        "    similarSection.style.display = 'block';\n",
        "\n",
        "    if (data && data.length > 0) {\n",
        "        let similarHtml = '<h4>ìœ ì‚¬ ì¶”ì²œ ë‰´ìŠ¤:</h4><div class=\"similar-news-grid\">';\n",
        "        data.forEach(similar => {\n",
        "            // ìœ ì‚¬ ì¶”ì²œ ë‰´ìŠ¤ë„ ìš”ì•½ê³¼ í•¨ê»˜ í‘œì‹œ\n",
        "            // ìœ ì‚¬ë„ ì ìˆ˜ (similarity_score) ì¶”ê°€ í‘œì‹œ\n",
        "            const score = similar.similarity_score ? `[ìœ ì‚¬ë„: ${similar.similarity_score}]` : '';\n",
        "            similarHtml += `\n",
        "                <div class=\"similar-news-card\">\n",
        "                    <h5>${similar.title} ${score}</h5>\n",
        "                    <p>${similar.preview_summary}</p>\n",
        "                    <div class=\"meta\">${similar.publisher} | <a href=\"${similar.link}\" target=\"_blank\" onclick=\"event.stopPropagation();\">ì›ë¬¸</a></div>\n",
        "                </div>\n",
        "            `;\n",
        "        });\n",
        "        similarHtml += '</div>';\n",
        "        similarSection.innerHTML = similarHtml;\n",
        "    } else {\n",
        "        similarSection.innerHTML = '<p>ìœ ì‚¬ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>';\n",
        "    }\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "    \"\"\"\n",
        "    return render_template_string(TEST_PAGE_HTML)\n",
        "\n",
        "# -----------------------------\n",
        "# ì„œë²„ ì‹¤í–‰\n",
        "# -----------------------------\n",
        "def run_app():\n",
        "    app.run(host='0.0.0.0', port=8000, debug=False, use_reloader=False)\n",
        "\n",
        "if NGROK_AUTHTOKEN:\n",
        "    os.system(f\"ngrok config add-authtoken {NGROK_AUTHTOKEN}\")\n",
        "    print(\"ngrok auth token ì„¤ì • ì™„ë£Œ!\")\n",
        "\n",
        "    server_thread = threading.Thread(target=run_app, daemon=True)\n",
        "    server_thread.start()\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    try:\n",
        "        public_url_obj = ngrok.connect(8000)\n",
        "        public_url = public_url_obj.public_url # NgrokTunnel ê°ì²´ì—ì„œ public_url ì†ì„±ë§Œ ì¶”ì¶œ\n",
        "\n",
        "        # âš¡ï¸ Fix 2: ê¹”ë”í•œ URL ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ë³€ê²½\n",
        "        print(f\"\\n=======================================================\")\n",
        "        print(f\"ğŸ‰ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ! ì•„ë˜ URLë¡œ ì ‘ì†í•˜ì„¸ìš”:\")\n",
        "        print(f\"ğŸŒ Ngrok URL: {public_url}\")\n",
        "        print(f\"ğŸŒ Ngrok URL: {public_url}/test\")\n",
        "        print(f\"=======================================================\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Ngrok ì—°ê²° ì˜¤ë¥˜: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiPNyxnEnVbn",
        "outputId": "e7c07ed6-6ff4-4645-c08f-d51185944fb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.12/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.32.4)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.12)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2025.10.5)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.0)\n",
            "ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ! (ì‹œê°„ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)\n",
            "KoBERT/T5 ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… KoBERT/T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: lcw99/t5-base-korean-text-summary, EOS Token ID: 1\n",
            "ngrok auth token ì„¤ì • ì™„ë£Œ!\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8000\n",
            " * Running on http://172.28.0.12:8000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "ğŸ‰ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ! ì•„ë˜ URLë¡œ ì ‘ì†í•˜ì„¸ìš”:\n",
            "ğŸŒ Ngrok URL: https://8f09be0ed76a.ngrok-free.app\n",
            "ğŸŒ Ngrok URL: https://8f09be0ed76a.ngrok-free.app/test\n",
            "=======================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}