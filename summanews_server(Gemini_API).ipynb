{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoyoung000/summanews/blob/main/summanews_server(Gemini_API).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë’¤ì§€ê²Ÿë‹¤ìŠ¤ë°œã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹ã„¹"
      ],
      "metadata": {
        "id": "tT0tqL4_sT6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flask flask-cors requests beautifulsoup4 pyngrok lxml newspaper3k lxml_html_clean scikit-learn trafilatura\n",
        "!pip install -U newspaper3k\n",
        "\n",
        "print(\"ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "\n",
        "# -----------------------------\n",
        "# ì„œë²„ ë° í™˜ê²½ ì„¤ì •\n",
        "# -----------------------------\n",
        "from flask import Flask, jsonify, request, render_template_string\n",
        "from flask_cors import CORS\n",
        "import os, requests, json, time, re, random, threading\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "from newspaper import Article\n",
        "import concurrent.futures\n",
        "import trafilatura\n",
        "import traceback\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# --- ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Colab ë¹„ë™ê¸° ê´€ë ¨\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì‹¤ì œ ê°’ì€ ìˆ¨ê¹€)\n",
        "os.environ.setdefault('NAVER_CLIENT_ID', 'fe9DLGhYbEVLy4sdQnVk')\n",
        "os.environ.setdefault('NAVER_CLIENT_SECRET', '2f0NEntTNN')\n",
        "NGROK_AUTHTOKEN = \"35b1S5mOVok0f5nGSBrHtyPrL0F_2mTpmmbP3SNLQ6yqYNdtb\"\n",
        "GEMINI_API_KEY = \"AIzaSyBH1jizfuH0v2hoe9Xvwup_x9IGlLOJ_-I\"\n",
        "GEMINI_MODEL_NAME = \"gemini-2.5-flash\"\n",
        "\n",
        "CATEGORIES = {\n",
        "    'ì •ì¹˜': 'politics',\n",
        "    'ê²½ì œ': 'business',\n",
        "    'ì‚¬íšŒ': 'society',\n",
        "    'ìƒí™œë¬¸í™”': 'life',\n",
        "    'ì—°ì˜ˆ': 'entertainment',\n",
        "    'ìŠ¤í¬ì¸ ': 'sports',\n",
        "    'ITê³¼í•™': 'technology',\n",
        "    'ì„¸ê³„': 'world',\n",
        "    'ë‚ ì”¨': 'weather',\n",
        "    'ì˜¤ëŠ˜ì˜ì¶”ì²œ': 'today'\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# âš¡ï¸ Gemini API í˜¸ì¶œ í•¨ìˆ˜\n",
        "# -----------------------------\n",
        "def call_gemini_api(text):\n",
        "    \"\"\"Gemini APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"ë„ˆëŠ” ë‰´ìŠ¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•´ì„œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ ìš”ì•½ê°€ì•¼. \"\n",
        "        \"ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ì˜¤ì§ ì‚¬ì‹¤ê³¼ í•µì‹¬ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì™„ë²½í•˜ê²Œ íŒŒì•…í•´. \"\n",
        "        \"íŒŒì•…í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **300ì ì´ë‚´**ì˜ ê°„ê²°í•˜ê³  ìœ ìµí•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´. \"\n",
        "        \"ìš”ì•½ë¬¸ì€ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•˜ë©°, ë‹¨ì–´ ë‹¨ìœ„ë¡œ ëë‚˜ì§€ ì•Šë„ë¡ ìœ ì˜í•´. \"\n",
        "        \"**ìš”ì•½ë¬¸ë§Œ ì¶œë ¥í•´ì•¼ í•˜ë©°, ì œëª©, ì„œë¡ ì ì¸ ë¬¸êµ¬, ê°ìƒ ë“± ì–´ë– í•œ ë¶€ê°€ì ì¸ í…ìŠ¤íŠ¸ë„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ.**\"\n",
        "    )\n",
        "    user_query = text\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_query}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        # --------------------------------------------------------------\n",
        "        # ğŸš¨ [í•µì‹¬ ìˆ˜ì •] ì•ˆì „ ì„¤ì • (Safety Settings) ì¶”ê°€\n",
        "        # ë‰´ìŠ¤ ê¸°ì‚¬(ì •ì¹˜/ì‚¬íšŒ)ê°€ ì°¨ë‹¨ë˜ì§€ ì•Šë„ë¡ ëª¨ë“  í•„í„°ë¥¼ í•´ì œí•©ë‹ˆë‹¤.\n",
        "        # --------------------------------------------------------------\n",
        "        \"safetySettings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}\n",
        "        ],\n",
        "        # --------------------------------------------------------------\n",
        "        \"generationConfig\": {\n",
        "            \"maxOutputTokens\": 1500,\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "    }\n",
        "\n",
        "    max_retries = 2\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers={'Content-Type': 'application/json'}, json=payload, timeout=20)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"\\n[ë””ë²„ê¹…] API ìš”ì²­ ì‹¤íŒ¨ (Status: {response.status_code})\")\n",
        "                print(f\"ğŸ” êµ¬ê¸€ ì—ëŸ¬ ë©”ì‹œì§€: {response.text}\")\n",
        "                return None\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            candidate = result.get('candidates', [{}])[0]\n",
        "\n",
        "            # [ë””ë²„ê¹…] ì°¨ë‹¨ ì›ì¸ í™•ì¸ (ì„±ê³µ ì¼€ì´ìŠ¤ ë‚´ì—ì„œì˜ ì°¨ë‹¨)\n",
        "            finish_reason = candidate.get('finishReason')\n",
        "            if finish_reason and finish_reason != \"STOP\":\n",
        "                print(f\"âš ï¸ Gemini ìš”ì•½ ì¤‘ë‹¨ë¨ (ì´ìœ : {finish_reason})\")\n",
        "                return None\n",
        "\n",
        "            if candidate.get('content') and candidate['content'].get('parts'):\n",
        "                summary = candidate['content']['parts'][0].get('text', '').strip()\n",
        "                return summary\n",
        "\n",
        "            return None\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(0.5)\n",
        "            else:\n",
        "                print(f\"âŒ Gemini API ìµœì¢… ì‹¤íŒ¨: {e}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Gemini API ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ìµœì¢… ì˜¤ë¥˜: {e}\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "# -----------------------------\n",
        "# í…ìŠ¤íŠ¸ ì •ì œ ìœ í‹¸ë¦¬í‹°\n",
        "# -----------------------------\n",
        "def clean_text(html_text):\n",
        "    \"\"\"HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ê³µë°±ì„ ì •ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'<[^>]+>', '', html_text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# -----------------------------\n",
        "# ê¸°ì‚¬ ë‚´ìš© ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# -----------------------------\n",
        "def get_article_text(url):\n",
        "    \"\"\"ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (Trafilatura -> newspaper3k í´ë°±).\"\"\"\n",
        "    initial_text = None\n",
        "    used_fallback = False  # í´ë°± ì—¬ë¶€ ë³€ìˆ˜ëŠ” ìœ ì§€ (ë””ë²„ê¹…ìš©)\n",
        "\n",
        "    try:\n",
        "        # 1. Trafilatura ì‹œë„ (1ìˆœìœ„)\n",
        "        downloaded = trafilatura.fetch_url(url)\n",
        "        if downloaded:\n",
        "            text = trafilatura.extract(downloaded, favor_recall=True, include_comments=False, target_language='ko', output_format='json')\n",
        "            if text:\n",
        "                try:\n",
        "                    text_json = json.loads(text)\n",
        "                    initial_text = text_json.get('text', '') or text_json.get('title', '')\n",
        "                except json.JSONDecodeError:\n",
        "                    initial_text = text\n",
        "\n",
        "        # 2. newspaper3k í´ë°± (2ìˆœìœ„)\n",
        "        if not initial_text or len(initial_text) < 100:\n",
        "            used_fallback = True\n",
        "            article = Article(url, language='ko')\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            initial_text = article.text\n",
        "\n",
        "        if not initial_text:\n",
        "            return None\n",
        "\n",
        "        text = initial_text\n",
        "\n",
        "        # -----------------------------------------------------------\n",
        "        # 3. ê°•ë ¥í•œ í´ë¦¬ë‹ (ì¡°ê±´ ì—†ì´ ëª¨ë“  í…ìŠ¤íŠ¸ì— ì ìš©)\n",
        "        # -----------------------------------------------------------\n",
        "\n",
        "        patterns_to_remove = [\n",
        "            r'\\[.*?\\]', r'\\s*\\(.*?=.*?\\)\\s*', r'[ê°€-í£]{2,4}\\s*ê¸°ì',\n",
        "            r'ê¸°ì = [ê°€-í£]{2,4}', r'â“’\\s*.*?ë¬´ë‹¨ì „ì¬\\s*ë°\\s*ì¬ë°°í¬\\s*ê¸ˆì§€',\n",
        "            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', r'ë‹¤ì‹œë³´ê¸°',\n",
        "            r'\\([^)]*=\\s*\\w+@\\w+\\.\\w+\\)', r'ì‚¬ì§„=.*?ì œê³µ', r'[\\s\\n]*\\(ì¶œì²˜:.*?\\)',\n",
        "            r'â–¶\\s*ë°”ë¡œê°€ê¸°.*', r'â€»\\s*.*\\s*ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€',\n",
        "            r'ì €ì‘ê¶Œì\\(c\\).*', r'Copyright.*',\n",
        "            r'[ê°€-í£]{2,4}ì´/ê°€ [ê°€-í£]{2,4}ì—ê²Œ ë“œë¦¬ëŠ” í•œë§ˆë””',\n",
        "            r'â–²â–³\\s*.*', r'Â©', r'\\[ì‚¬ì§„\\s*.*?\\]', r'\\[ì´ë¯¸ì§€ì¶œì²˜.*?\\]',\n",
        "            r'[â– â—†â—]', r'\\s*[ê°€-í£]{2,4}\\s*(íŠ¹íŒŒì›|ê¸°ì)\\s*=\\s*',\n",
        "            r'^\\s*\\[(ë‹¨ë…|ì¢…í•©|ì†ë³´|ì•µì»¤|ì˜ìƒ)\\]\\s*',\n",
        "        ]\n",
        "        for pattern in patterns_to_remove:\n",
        "            text = re.sub(pattern, '', text).strip()\n",
        "\n",
        "        text = re.sub(r'^\\s*\\(.*?\\)\\s*=\\s*', '', text).strip() # (ì§€ì—­=ë‰´ìŠ¤) ì œê±°\n",
        "\n",
        "        # ê¼¬ë¦¬ë§(ê¸°ì‚¬ ëë¶€ë¶„ ë¶ˆí•„ìš” ë¬¸êµ¬) ì œê±°\n",
        "        sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', text)\n",
        "        cleaned_sentences = sentences[:]\n",
        "        ending_patterns = [\n",
        "            r'\\(ë\\)\\s*', r'\\(ìë£Œì‚¬ì§„=.*?\\)', r'ì§€ê¸ˆê¹Œì§€ [ê°€-í£]{2,4}ì˜€ìŠµë‹ˆë‹¤.',\n",
        "            r'ì¬í¸ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.', r'ìì„¸í•œ ë‚´ìš©ì€ .*ì—ì„œ í™•ì¸í•˜ì„¸ìš”.', r'â–¶.*'\n",
        "        ]\n",
        "\n",
        "        if len(sentences) > 5:\n",
        "            removed_count = 0\n",
        "            for i in range(1, min(6, len(sentences))):\n",
        "                last = cleaned_sentences[-1] if cleaned_sentences else \"\"\n",
        "                is_ending = any(re.search(p, last) for p in ending_patterns)\n",
        "                is_junk = re.search(r'(ì‚¬ì§„|ì œê³µ|ë°°í¬|ë‰´ìŠ¤|ê¸°ì|ë°”ë¡œê°€ê¸°|í›„ì›)', last) and len(last) < 40\n",
        "\n",
        "                if is_ending or is_junk or (len(last) < 30 and len(cleaned_sentences) > 5 and removed_count < 3):\n",
        "                    cleaned_sentences.pop()\n",
        "                    removed_count += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        final_text = re.sub(r'\\s+', ' ', \" \".join(cleaned_sentences)).strip()\n",
        "        return final_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e} (URL: {url})\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# âœ… ìš”ì•½ í•¨ìˆ˜ (Gemini + Fallback)\n",
        "# -----------------------------\n",
        "def make_summary(text):\n",
        "    \"\"\"Gemini APIë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ ë£° ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\"\"\"\n",
        "    text = (text or \"\").strip()\n",
        "    if not text: return \"\"\n",
        "\n",
        "    # ì „ì²˜ë¦¬\n",
        "    text = re.sub(r'[\\u4E00-\\u9FFF]+', '', text)\n",
        "    text = re.sub(r'[\\(\\[].*?ê¸°ì[\\]\\)]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Gemini ì‹œë„\n",
        "    summary = call_gemini_api(text)\n",
        "\n",
        "    if summary:\n",
        "        if not summary.endswith(('.', '!', '?', 'â€¦')): summary += '.'\n",
        "        # ğŸš¨ ì„±ê³µ ì‹œ íƒœê·¸ ë¶€ì°©\n",
        "        return f\"[AI ìš”ì•½] {summary}\"\n",
        "\n",
        "    # Fallback (ë£° ê¸°ë°˜)\n",
        "    print(\"ğŸ”» ìš”ì•½ Fallback ì‹¤í–‰\")\n",
        "    sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', text)\n",
        "    fallback = \" \".join(sentences[:3])\n",
        "    if len(fallback) > 300: fallback = fallback[:300] + \"...\"\n",
        "    if not fallback.endswith(('.', '!', '?', 'â€¦')): fallback += \".\"\n",
        "    # ğŸš¨ ì‹¤íŒ¨ ì‹œ íƒœê·¸ ë¶€ì°©\n",
        "    return f\"[ê¸°ë³¸ ìš”ì•½] {fallback}\"\n",
        "\n",
        "def extract_image_and_views(url, timeout=6):\n",
        "    \"\"\"ì´ë¯¸ì§€ì™€ ì¡°íšŒìˆ˜ ì¶”ì¶œ.\"\"\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "        if resp.status_code != 200: return None, None\n",
        "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "\n",
        "        image = None\n",
        "        img_selectors = [\n",
        "            ('meta[property=\"og:image\"]', 'content'), ('meta[name=\"twitter:image\"]', 'content'),\n",
        "            ('img[class*=\"thumb\"]', 'src'), ('img[class*=\"photo\"]', 'src'), ('article img', 'src')\n",
        "        ]\n",
        "        for sel, attr in img_selectors:\n",
        "            tag = soup.select_one(sel)\n",
        "            if tag:\n",
        "                image = tag.get(attr)\n",
        "                if image and image.startswith('//'): image = 'https:' + image\n",
        "                if image: break\n",
        "\n",
        "        if not image:\n",
        "            try:\n",
        "                art = Article(url)\n",
        "                art.download(); art.parse()\n",
        "                image = art.top_image\n",
        "            except: pass\n",
        "\n",
        "        text = soup.get_text(separator=' ')\n",
        "        views = None\n",
        "        m = re.search(r'(ì¡°íšŒìˆ˜|ì½ìŒ|Views)[^\\d]*([\\d,]+)', text, re.IGNORECASE)\n",
        "        if m:\n",
        "            views = int(re.sub(r'[^0-9]', '', m.group(2)))\n",
        "\n",
        "        return image, views\n",
        "    except: return None, None\n",
        "\n",
        "def extract_publisher_name(url):\n",
        "    \"\"\"ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì •.\"\"\"\n",
        "    try:\n",
        "        publisher_map = {\n",
        "            'chosun.com': 'ì¡°ì„ ì¼ë³´', 'donga.com': 'ë™ì•„ì¼ë³´', 'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',\n",
        "            'hankyung.com': 'í•œêµ­ê²½ì œ', 'maeil.com': 'ë§¤ì¼ê²½ì œ', 'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',\n",
        "            'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸', 'news1.kr': 'ë‰´ìŠ¤1', 'hani.co.kr': 'í•œê²¨ë ˆ', 'kbs.co.kr': 'KBS'\n",
        "        }\n",
        "        for domain, name in publisher_map.items():\n",
        "            if domain in url: return name\n",
        "        if \"naver.com\" in url: return \"ë„¤ì´ë²„ ë‰´ìŠ¤\"\n",
        "        return \"ì•Œ ìˆ˜ ì—†ìŒ\"\n",
        "    except: return \"ì•Œ ìˆ˜ ì—†ìŒ\"\n",
        "\n",
        "# -----------------------------\n",
        "# ë‰´ìŠ¤ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤\n",
        "# -----------------------------\n",
        "class NewsService:\n",
        "    def __init__(self):\n",
        "        self.naver_client_id = os.environ.get('NAVER_CLIENT_ID')\n",
        "        self.naver_client_secret = os.environ.get('NAVER_CLIENT_SECRET')\n",
        "        self.query_map = {\n",
        "            'ì •ì¹˜': 'ì •ì¹˜', 'ê²½ì œ': 'ê²½ì œ', 'ì‚¬íšŒ': 'ì‚¬íšŒ', 'ìƒí™œë¬¸í™”': 'ìƒí™œ ë¬¸í™”', 'ì—°ì˜ˆ': 'ì—°ì˜ˆ',\n",
        "            'ìŠ¤í¬ì¸ ': 'ìŠ¤í¬ì¸ ', 'ITê³¼í•™': 'ITê³¼í•™', 'ì„¸ê³„': 'ì„¸ê³„ ë‰´ìŠ¤', 'ë‚ ì”¨': 'ë‚ ì”¨ ì˜ˆë³´', 'ì˜¤ëŠ˜ì˜ì¶”ì²œ': 'ë‰´ìŠ¤'\n",
        "        }\n",
        "\n",
        "    def get_naver_news(self, query, count=5, sort_type='sim'):\n",
        "        url = \"https://openapi.naver.com/v1/search/news.json\"\n",
        "        headers = {'X-Naver-Client-Id': self.naver_client_id, 'X-Naver-Client-Secret': self.naver_client_secret}\n",
        "        params = {'query': query, 'display': count, 'start': 1, 'sort': sort_type}\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers, params=params, timeout=6)\n",
        "            r.raise_for_status()\n",
        "            return r.json().get('items', [])\n",
        "        except: return []\n",
        "\n",
        "news_service = NewsService()\n",
        "\n",
        "# --- ìœ íš¨ì„± ê²€ì‚¬ ---\n",
        "def validate_text_relevance(title, full_text):\n",
        "    if not full_text or not title: return False\n",
        "    clean_title = re.sub(r'\\[.*?\\]|\\(.*?\\)|[^\\w\\s]', ' ', title).strip()\n",
        "    keywords = [w for w in clean_title.split() if len(w) > 1][:3]\n",
        "    if not keywords: return True\n",
        "    match_count = sum(1 for kw in keywords if kw in full_text)\n",
        "    return match_count >= max(1, len(keywords) // 2)\n",
        "\n",
        "# --- ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ---\n",
        "title_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
        "cached_titles = []\n",
        "cached_vectors = None\n",
        "\n",
        "def is_similar_title_exists(new_title, similarity_threshold=0.85):\n",
        "    global cached_titles, cached_vectors\n",
        "    clean_title = re.sub(r'\\[.*?\\]|\\(.*?\\)|[^\\w\\s]', ' ', new_title).strip()\n",
        "    if len(clean_title) < 5: return False\n",
        "\n",
        "    if not cached_titles:\n",
        "        cached_titles.append(clean_title)\n",
        "        cached_vectors = title_vectorizer.fit_transform([clean_title])\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        new_vector = title_vectorizer.transform([clean_title])\n",
        "        similarities = cosine_similarity(new_vector, cached_vectors).flatten()\n",
        "        if np.any(similarities >= similarity_threshold): return True\n",
        "\n",
        "        cached_titles.append(clean_title)\n",
        "        from scipy.sparse import vstack\n",
        "        cached_vectors = vstack([cached_vectors, new_vector])\n",
        "        return False\n",
        "    except:\n",
        "        cached_titles.append(clean_title)\n",
        "        return False\n",
        "\n",
        "# -----------------------------\n",
        "# âš¡ï¸ ìˆ˜ì •ëœ ë°ì´í„° ì •ê·œí™” (ìš”ì•½ ì•ˆí•¨)\n",
        "# -----------------------------\n",
        "def normalize_and_enrich(items, source='naver', category='ì¼ë°˜'):\n",
        "    \"\"\"ë‰´ìŠ¤ í•­ëª©ì„ ì •ê·œí™”í•˜ê³  ë³¸ë¬¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. (ìš”ì•½ì€ ì„ ë³„ í›„ ì§„í–‰)\"\"\"\n",
        "    item = items[0]\n",
        "    title = clean_text(item.get('title') or '')\n",
        "    description = clean_text(item.get('description') or '')\n",
        "    link = item.get('link')\n",
        "    full_text = None\n",
        "\n",
        "    pub_date = item.get('pubDate')\n",
        "    try:\n",
        "        dt = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z')\n",
        "        pub_date = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    except: pass\n",
        "\n",
        "    try:\n",
        "        # ì œëª© ë³´ê°•\n",
        "        if link and (not title or not description):\n",
        "            try:\n",
        "                ta = Article(link, language='ko'); ta.download(); ta.parse()\n",
        "                if ta.title and len(ta.title) > len(title): title = clean_text(ta.title)\n",
        "            except: pass\n",
        "\n",
        "        # 1. ë³¸ë¬¸ ì¶”ì¶œ\n",
        "        full_text = get_article_text(link)\n",
        "\n",
        "        # 2. í•„í„°ë§ (300ì ë¯¸ë§Œ, ìœ íš¨ì„± ê²€ì‚¬)\n",
        "        if not full_text or len(full_text) < 300: return None\n",
        "        if not validate_text_relevance(title, full_text): return None\n",
        "\n",
        "        # 3. ë©”íƒ€ë°ì´í„° (ìš”ì•½ì€ ë¹„ì›Œë‘ )\n",
        "        publisher = extract_publisher_name(link)\n",
        "        img, views = extract_image_and_views(link)\n",
        "\n",
        "        return {\n",
        "            \"title\": title, \"description\": description, \"link\": link, \"pub_date\": pub_date,\n",
        "            \"image_url\": img, \"view_count\": str(views) if views else \"None\",\n",
        "            \"preview_summary\": \"\", \"detailed_summary\": \"\",\n",
        "            \"source\": source, \"category\": category, \"publisher\": publisher,\n",
        "            \"full_text\": full_text\n",
        "        }\n",
        "    except: return None\n",
        "\n",
        "# --- ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ---\n",
        "def find_similar_news(main_article_content, main_link, category, count=3):\n",
        "    # ê²€ìƒ‰ (ë™ì¼ ì¹´í…Œê³ ë¦¬)\n",
        "    raw_items = news_service.get_naver_news(news_service.query_map.get(category, category), count=5, sort_type='sim')\n",
        "\n",
        "    # ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ\n",
        "    processed_candidates = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as exc:\n",
        "        futures = {exc.submit(normalize_and_enrich, [item], 'naver', category) for item in raw_items if item['link'] != main_link}\n",
        "        for f in concurrent.futures.as_completed(futures):\n",
        "            res = f.result()\n",
        "            if res: processed_candidates.append(res)\n",
        "\n",
        "    if not processed_candidates: return []\n",
        "\n",
        "    # ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    corpus = [main_article_content] + [p['full_text'] for p in processed_candidates]\n",
        "    vec = TfidfVectorizer().fit_transform(corpus)\n",
        "    sims = cosine_similarity(vec[0:1], vec[1:]).flatten()\n",
        "\n",
        "    indices = np.argsort(sims)[::-1]\n",
        "    similar_news = []\n",
        "\n",
        "    # ìƒìœ„ 3ê°œë§Œ ìš”ì•½ ì§„í–‰\n",
        "    found = 0\n",
        "    for i in indices:\n",
        "        if sims[i] > 0.2:\n",
        "            candidate = processed_candidates[i]\n",
        "            # âš¡ï¸ ìœ ì‚¬ ë‰´ìŠ¤ë„ ì—¬ê¸°ì„œ ìš”ì•½ ìˆ˜í–‰ (ì‚¬ìš©ìê°€ ë³¼ ê²ƒì´ë¯€ë¡œ)\n",
        "            summary = make_summary(candidate['full_text'])\n",
        "\n",
        "            # ë¯¸ë¦¬ë³´ê¸° ìƒì„±\n",
        "            sents = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', summary)\n",
        "            prev = \" \".join(sents[:2])\n",
        "            if len(prev) > 50: prev = prev[:50] + \"...\"\n",
        "\n",
        "            candidate['detailed_summary'] = summary\n",
        "            candidate['preview_summary'] = prev\n",
        "            candidate['similarity_score'] = f\"{sims[i]:.2f}\"\n",
        "            del candidate['full_text'] # ì „ì†¡ ì „ ì‚­ì œ\n",
        "\n",
        "            similar_news.append(candidate)\n",
        "            found += 1\n",
        "            if found >= count: break\n",
        "\n",
        "    return similar_news\n",
        "\n",
        "# -----------------------------\n",
        "# Flask API\n",
        "# -----------------------------\n",
        "@app.after_request\n",
        "def after_request(r):\n",
        "    r.headers.add('Access-Control-Allow-Origin', '*')\n",
        "    r.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
        "    r.headers.add('Access-Control-Allow-Methods', 'GET,POST,OPTIONS')\n",
        "    return r\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return jsonify({\"message\": \"ğŸ—ï¸ ë‰´ìŠ¤ ì¶”ì²œ AI ì„œë²„ (Optimized)\", \"status\": \"running\"})\n",
        "\n",
        "@app.route('/api/news', defaults={'category': None})\n",
        "@app.route('/api/news/<category>')\n",
        "def api_news(category):\n",
        "    global cached_titles, cached_vectors\n",
        "    start_time = time.time()\n",
        "\n",
        "    # íŒŒë¼ë¯¸í„° íŒŒì‹±\n",
        "    cats_param = request.args.get('categories')\n",
        "    selected_cats = cats_param.split(',') if cats_param else []\n",
        "    inc_pubs = [p.strip() for p in request.args.get('include_publishers', '').split(',') if p.strip()]\n",
        "    exc_pubs = [p.strip() for p in request.args.get('exclude_publishers', '').split(',') if p.strip()]\n",
        "\n",
        "    # ì´ˆê¸°í™”\n",
        "    cached_titles = []\n",
        "    cached_vectors = None\n",
        "    unique_links = set()\n",
        "    all_raw_items = []\n",
        "\n",
        "    # ëª¨ë“œ ì„¤ì •\n",
        "    is_today = category == 'ì˜¤ëŠ˜ì˜ì¶”ì²œ' or (category is None and selected_cats)\n",
        "\n",
        "    if is_today:\n",
        "        targets = selected_cats if selected_cats else [k for k in CATEGORIES.keys() if k != 'ì˜¤ëŠ˜ì˜ì¶”ì²œ']\n",
        "        sort_type = 'sim'\n",
        "    else:\n",
        "        targets = [category] if category else [k for k in CATEGORIES.keys() if k != 'ì˜¤ëŠ˜ì˜ì¶”ì²œ']\n",
        "        sort_type = 'date'\n",
        "\n",
        "    # 1. ìˆ˜ì§‘\n",
        "    for cat in targets:\n",
        "        query = news_service.query_map.get(cat, cat)\n",
        "        items = news_service.get_naver_news(query, count=10, sort_type=sort_type)\n",
        "        for item in items:\n",
        "            link = item.get('link')\n",
        "            title = clean_text(item.get('title') or '')\n",
        "            if link in unique_links: continue\n",
        "            unique_links.add(link)\n",
        "            if is_similar_title_exists(title): continue\n",
        "            all_raw_items.append((item, 'naver', cat))\n",
        "\n",
        "    # 2. ë³‘ë ¬ í¬ë¡¤ë§ (ìš”ì•½ X)\n",
        "    candidates = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as exc:\n",
        "        futures = {exc.submit(normalize_and_enrich, [r], s, c) for r, s, c in all_raw_items}\n",
        "        for f in concurrent.futures.as_completed(futures):\n",
        "            res = f.result()\n",
        "            if res: candidates.append(res)\n",
        "\n",
        "    # 3. ì–¸ë¡ ì‚¬ í•„í„°ë§\n",
        "    filtered = []\n",
        "    inc_set = set(inc_pubs)\n",
        "    exc_set = set(exc_pubs)\n",
        "    for item in candidates:\n",
        "        pub = item.get('publisher')\n",
        "        if inc_set and pub and pub not in inc_set: continue\n",
        "        if exc_set and pub and pub in exc_set: continue\n",
        "        filtered.append(item)\n",
        "\n",
        "    # 4. ì„ ë³„ (Top 3)\n",
        "    final_selection = []\n",
        "    if is_today and targets:\n",
        "        # ì¹´í…Œê³ ë¦¬ë³„ 1ë“± ë½‘ê³  ëœë¤ 3ê°œ\n",
        "        by_cat = {c: [] for c in targets}\n",
        "        for item in filtered:\n",
        "            if item['category'] in by_cat: by_cat[item['category']].append(item)\n",
        "\n",
        "        headlines = []\n",
        "        for cat in targets:\n",
        "            srt = sorted(by_cat[cat], key=lambda x: (x.get('pub_date') or '', x.get('view_count') or '0'), reverse=True)\n",
        "            if srt: headlines.append(srt[0])\n",
        "\n",
        "        final_selection = random.sample(headlines, min(3, len(headlines)))\n",
        "    else:\n",
        "        # ë‚ ì§œìˆœ 3ê°œ\n",
        "        final_selection = sorted(filtered, key=lambda x: x.get('pub_date') or '', reverse=True)[:3]\n",
        "\n",
        "    # 5. ğŸš€ Gemini ìš”ì•½ (ì„ ë³„ëœ ê²ƒë§Œ)\n",
        "    print(f\"ğŸš€ [API ì ˆì•½] ìµœì¢… {len(final_selection)}ê°œ ê¸°ì‚¬ ìš”ì•½ ì‹œì‘...\")\n",
        "    for item in final_selection:\n",
        "        full_txt = item['full_text']\n",
        "\n",
        "        # ìš”ì•½ í˜¸ì¶œ\n",
        "        summary = make_summary(full_txt)\n",
        "        item['detailed_summary'] = summary\n",
        "\n",
        "        # ë¯¸ë¦¬ë³´ê¸°\n",
        "        sents = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+', summary)\n",
        "        prev = \" \".join(sents[:2])\n",
        "        if len(prev) > 50: prev = prev[:50].strip() + \"...\"\n",
        "        elif not prev.endswith('.'): prev += \"...\"\n",
        "        item['preview_summary'] = prev\n",
        "\n",
        "        del item['full_text'] # ì „ì†¡ ìµœì í™”\n",
        "\n",
        "    print(f\"â±ï¸ ì²˜ë¦¬ ì™„ë£Œ: {time.time()-start_time:.2f}ì´ˆ\")\n",
        "    return jsonify(final_selection)\n",
        "\n",
        "@app.route('/api/similar_news')\n",
        "def api_similar_news():\n",
        "    main_link = request.args.get('link')\n",
        "    category = request.args.get('category', 'all')\n",
        "    if not main_link: return jsonify({\"error\": \"No link\"}), 400\n",
        "\n",
        "    # 1. ë©”ì¸ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (ìš”ì•½ ì•ˆëœ ìƒíƒœ)\n",
        "    main_article = normalize_and_enrich([{'link': main_link, 'title': '', 'description': '', 'pubDate': ''}], 'naver', category)\n",
        "\n",
        "    if not main_article or not main_article.get('full_text'):\n",
        "        return jsonify({\"detailed_summary\": \"ê¸°ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\", \"similar_news\": []}), 500\n",
        "\n",
        "    # 2. âš¡ï¸ ë©”ì¸ ê¸°ì‚¬ ìš”ì•½ ìˆ˜í–‰ (ì—¬ê¸°ì„œ ìˆ˜í–‰)\n",
        "    if not main_article['detailed_summary']:\n",
        "        main_article['detailed_summary'] = make_summary(main_article['full_text'])\n",
        "\n",
        "    # 3. ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰\n",
        "    sim_news = find_similar_news(main_article['full_text'], main_link, category)\n",
        "\n",
        "    return jsonify({\n",
        "        \"detailed_summary\": main_article['detailed_summary'],\n",
        "        \"similar_news\": sim_news\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Flask TEST Page\n",
        "# -----------------------------\n",
        "# -----------------------------\n",
        "# Flask TEST Page (ë²„íŠ¼ í´ë¦­ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ)\n",
        "# -----------------------------\n",
        "\n",
        "@app.route('/test')\n",
        "def test_page():\n",
        "    \"\"\"ì›¹ í…ŒìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.\"\"\"\n",
        "    # JavaScript ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë¥¼ DOMContentLoaded ë‚´ë¶€ì— ë°°ì¹˜í•˜ì—¬ ì•ˆì •ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.\n",
        "    TEST_PAGE_HTML = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "<html lang=\"ko\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "<title>ë‰´ìŠ¤ AI ì„œë²„ í…ŒìŠ¤íŠ¸</title>\n",
        "<style>\n",
        "body{font-family:Segoe UI,Roboto,Arial;background-color:#ffffff;margin:0;padding:20px}\n",
        ".container{max-width:1100px;margin:0 auto;background:#fff;border-radius:10px;overflow:hidden;box-shadow:0 10px 30px rgba(0,0,0,.15)}\n",
        ".header{background:#2c3e50;color:#fff;padding:18px;text-align:center}\n",
        ".controls{padding:16px;background:#f8f9fa;border-bottom:1px solid #e9ecef}\n",
        ".input-group{display:flex;gap:10px;align-items:center;flex-wrap:wrap}\n",
        "select,input{padding:8px;border-radius:6px;border:1px solid #ddd}\n",
        ".btn{padding:4px 8px;border-radius:6px;color:#fff;border:none;cursor:pointer;transition:background-color .2s ease; margin: 2px 2px;}\n",
        ".btn-primary{background:#007bff}\n",
        ".btn-success{background:#2ecc71}\n",
        ".btn-danger{background:#e74c3c}\n",
        ".results{padding:16px}\n",
        ".news-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(320px,1fr));gap:16px}\n",
        ".news-card{border:1px solid #eee;border-radius:8px;overflow:hidden;background:#fff;position:relative; cursor:pointer; transition: box-shadow 0.3s;}\n",
        ".news-card:hover{box-shadow: 0 4px 15px rgba(0,0,0,.1);}\n",
        ".news-card.active { box-shadow: 0 0 15px rgba(0, 123, 255, 0.5); border-color: #007bff; }\n",
        ".news-card img{width:100%;height:180px;object-fit:cover}\n",
        ".content{padding:12px}\n",
        ".meta{font-size:12px;color:#666;margin-top:8px}\n",
        ".filter-section h4{margin-top:0; margin-bottom: 5px;}\n",
        ".similar-news-section{margin-top:15px;padding-top:10px;border-top:1px dashed #ccc;}\n",
        ".similar-news-section h4{margin-top:0; color:#2c3e50;}\n",
        ".loading-spinner {border: 4px solid #f3f3f3; border-top: 4px solid #3498db; border-radius: 50%; width: 16px; height: 16px; animation: spin 2s linear infinite; display: inline-block; margin-right: 5px;}\n",
        "@keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }\n",
        ".similar-news-grid {display: grid; grid-template-columns: repeat(auto-fill, minmax(280px, 1fr)); gap: 10px; margin-top: 10px;}\n",
        ".similar-news-card {border: 1px solid #2ecc71; border-radius: 6px; padding: 10px; background: #f0fff7; font-size: 14px;}\n",
        ".similar-news-card h5 {margin-top: 0; margin-bottom: 5px; font-size: 15px; color: #2ecc71;}\n",
        ".similar-news-card p {margin: 0; font-size: 13px;}\n",
        "\n",
        "/* ì¶”ê°€ëœ CSS */\n",
        ".filter-section .btn {\n",
        "    margin: 2px; /* ë²„íŠ¼ ê°„ê²© ì¡°ì • */\n",
        "}\n",
        "\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\"><h2>ë‰´ìŠ¤ AI ì„œë²„ í…ŒìŠ¤íŠ¸ </h2><div>ì„œë²„ ì‹¤í–‰ì¤‘</div></div>\n",
        "    <div class=\"controls\">\n",
        "        <div style=\"margin-bottom:4px;\">\n",
        "            <label>ì¹´í…Œê³ ë¦¬:</label>\n",
        "            <select id=\"category\">\n",
        "                <option value=\"ì •ì¹˜\">ì •ì¹˜</option>\n",
        "                <option value=\"ê²½ì œ\">ê²½ì œ</option>\n",
        "                <option value=\"ì‚¬íšŒ\">ì‚¬íšŒ</option>\n",
        "                <option value=\"ìƒí™œë¬¸í™”\">ìƒí™œë¬¸í™”</option>\n",
        "                <option value=\"ì—°ì˜ˆ\">ì—°ì˜ˆ</option>\n",
        "                <option value=\"ìŠ¤í¬ì¸ \">ìŠ¤í¬ì¸ </option>\n",
        "                <option value=\"ì„¸ê³„\">ì„¸ê³„</option> <option value=\"ë‚ ì”¨\">ë‚ ì”¨</option> <option value=\"ITê³¼í•™\">ITê³¼í•™</option>\n",
        "                <option value=\"ì˜¤ëŠ˜ì˜ì¶”ì²œ\">ì˜¤ëŠ˜ì˜ì¶”ì²œ</option>\n",
        "            </select>\n",
        "            <button class=\"btn btn-primary\" onclick=\"fetchNews()\">ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° </button>\n",
        "        </div>\n",
        "        <div id=\"todayBtns\" style=\"margin-bottom:2px;\">\n",
        "            <label>ì˜¤ëŠ˜ì˜ì¶”ì²œ ì„ íƒ:</label>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì •ì¹˜\">ì •ì¹˜</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ê²½ì œ\">ê²½ì œ</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì‚¬íšŒ\">ì‚¬íšŒ</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ìƒí™œë¬¸í™”\">ìƒí™œë¬¸í™”</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì—°ì˜ˆ\">ì—°ì˜ˆ</button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ìŠ¤í¬ì¸ \">ìŠ¤í¬ì¸ </button>\n",
        "            <button class=\"btn btn-primary today-btn\" data-cat=\"ì„¸ê³„\">ì„¸ê³„</button> <button class=\"btn btn-primary today-btn\" data-cat=\"ë‚ ì”¨\">ë‚ ì”¨</button> <button class=\"btn btn-primary today-btn\" data-cat=\"ITê³¼í•™\">ITê³¼í•™</button>\n",
        "        </div>\n",
        "    </div>\n",
        "    <div class=\"results\">\n",
        "        <div id=\"status\"></div>\n",
        "        <div id=\"newsContainer\"></div>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "let newsData = null;\n",
        "let selectedTodayCategories = [];\n",
        "let includePublishers = new Set();\n",
        "let excludePublishers = new Set();\n",
        "let currentActiveCard = null;\n",
        "\n",
        "function updateSelectedDisplay(containerId, dataSet) {\n",
        "    const display = document.getElementById(containerId);\n",
        "    display.innerText = \"ì„ íƒë¨: \" + (dataSet.size > 0 ? Array.from(dataSet).join(', ') : 'ì—†ìŒ');\n",
        "}\n",
        "\n",
        "// *** í•µì‹¬ ìˆ˜ì •: ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë¥¼ DOMContentLoaded ë‚´ì— ë°°ì¹˜í•˜ì—¬ ì•ˆì •ì„± í™•ë³´ ***\n",
        "window.addEventListener('DOMContentLoaded', () => {\n",
        "\n",
        "    // ì˜¤ëŠ˜ì˜ì¶”ì²œ ë²„íŠ¼ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ\n",
        "    document.querySelectorAll('.today-btn').forEach(btn => {\n",
        "        btn.addEventListener('click', () => {\n",
        "            const cat = btn.dataset.cat;\n",
        "            if (selectedTodayCategories.includes(cat)) {\n",
        "                selectedTodayCategories = selectedTodayCategories.filter(c => c !== cat);\n",
        "                btn.classList.remove('btn-success');\n",
        "                btn.classList.add('btn-primary');\n",
        "            } else {\n",
        "                // ì´ì „ì— 'ê±´ê°•'ì´ì—ˆë˜ ë²„íŠ¼ë“¤ì€ 'ì„¸ê³„', 'ë‚ ì”¨'ë¡œ ëŒ€ì²´ë˜ì—ˆì§€ë§Œ data-cat ì†ì„±ì€ ì •í™•í•©ë‹ˆë‹¤.\n",
        "                selectedTodayCategories.push(cat);\n",
        "                btn.classList.add('btn-success');\n",
        "                btn.classList.remove('btn-primary');\n",
        "            }\n",
        "        });\n",
        "    });\n",
        "\n",
        "    // ì–¸ë¡ ì‚¬ í¬í•¨ ë²„íŠ¼ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ\n",
        "    document.querySelectorAll('.btn-include').forEach(btn => {\n",
        "        btn.addEventListener('click', () => {\n",
        "            const publisher = btn.dataset.publisher;\n",
        "            if (includePublishers.has(publisher)) {\n",
        "                includePublishers.delete(publisher);\n",
        "                btn.classList.remove('btn-success');\n",
        "                btn.classList.add('btn-primary');\n",
        "            } else {\n",
        "                if (excludePublishers.has(publisher)) {\n",
        "                    console.warn('í¬í•¨/ì œì™¸ í•„í„°ëŠ” ì¤‘ë³µ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');\n",
        "                    return;\n",
        "                }\n",
        "                includePublishers.add(publisher);\n",
        "                btn.classList.add('btn-success');\n",
        "                btn.classList.remove('btn-primary');\n",
        "            }\n",
        "            updateSelectedDisplay('selected-includes', includePublishers);\n",
        "        });\n",
        "    });\n",
        "\n",
        "    // ì–¸ë¡ ì‚¬ ì œì™¸ ë²„íŠ¼ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ\n",
        "    document.querySelectorAll('.btn-exclude').forEach(btn => {\n",
        "        btn.addEventListener('click', () => {\n",
        "            const publisher = btn.dataset.publisher;\n",
        "            if (excludePublishers.has(publisher)) {\n",
        "                excludePublishers.delete(publisher);\n",
        "                btn.classList.remove('btn-danger');\n",
        "                btn.classList.add('btn-primary');\n",
        "            } else {\n",
        "                if (includePublishers.has(publisher)) {\n",
        "                    console.warn('í¬í•¨/ì œì™¸ í•„í„°ëŠ” ì¤‘ë³µ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');\n",
        "                    return;\n",
        "                }\n",
        "                excludePublishers.add(publisher);\n",
        "                btn.classList.add('btn-danger');\n",
        "                btn.classList.remove('btn-primary');\n",
        "            }\n",
        "            updateSelectedDisplay('selected-excludes', excludePublishers);\n",
        "        });\n",
        "    });\n",
        "});\n",
        "// ------------------------------------\n",
        "\n",
        "function setStatus(msg, type='') {\n",
        "    const s = document.getElementById('status');\n",
        "    s.innerHTML = msg;\n",
        "    s.style.color = type==='error' ? '#c0392b' : '#2ecc71';\n",
        "}\n",
        "\n",
        "// --- 1. ë©”ì¸ ë‰´ìŠ¤ ëª©ë¡ ë¡œë“œ (ë¹ ë¥¸ ë¡œë”©) ---\n",
        "async function fetchNews() {\n",
        "    const startTime = new Date();\n",
        "    const cat = document.getElementById('category').value;\n",
        "\n",
        "    let endpoint = '/api/news';\n",
        "    const params = new URLSearchParams();\n",
        "\n",
        "    // í•„í„°ë§ íŒŒë¼ë¯¸í„° êµ¬ì„±\n",
        "    if (cat) { endpoint += '/' + encodeURIComponent(cat); }\n",
        "    if (selectedTodayCategories.length) { params.append('categories', selectedTodayCategories.join(',')); }\n",
        "    if (includePublishers.size > 0) { params.append('include_publishers', Array.from(includePublishers).join(',')); }\n",
        "    if (excludePublishers.size > 0) { params.append('exclude_publishers', Array.from(excludePublishers).join(',')); }\n",
        "    if (params.toString()) { endpoint += '?' + params.toString(); }\n",
        "\n",
        "    setStatus('ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...');\n",
        "    currentActiveCard = null; // í™œì„± ì¹´ë“œ ì´ˆê¸°í™”\n",
        "\n",
        "    try {\n",
        "        const resp = await fetch(endpoint);\n",
        "        const data = await resp.json();\n",
        "        newsData = data;\n",
        "        renderNews(newsData);\n",
        "\n",
        "        const endTime = new Date();\n",
        "        const elapsedTime = (endTime - startTime) / 1000;\n",
        "        setStatus(`âœ… ëª©ë¡ ë¡œë“œ ì™„ë£Œ (${elapsedTime.toFixed(2)}ì´ˆ ê±¸ë¦¼). ê¸°ì‚¬ë¥¼ í´ë¦­í•˜ë©´ ìœ ì‚¬ ë‰´ìŠ¤ê°€ ë¡œë“œë©ë‹ˆë‹¤.`);\n",
        "    } catch (e) {\n",
        "        setStatus('ìš”ì²­ ì‹¤íŒ¨: ' + e.message, 'error');\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- 2. ë‰´ìŠ¤ ì¹´ë“œ í´ë¦­ ì‹œ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ---\n",
        "async function fetchSimilarNews(link, category, cardElement) {\n",
        "\n",
        "    const similarSectionDiv = cardElement.querySelector('.similar-news-section');\n",
        "    const similarStatusDiv = cardElement.querySelector('.similar-status');\n",
        "\n",
        "    // 1. í† ê¸€ ê¸°ëŠ¥: ì´ë¯¸ ë¡œë“œë˜ì—ˆê³  í˜„ì¬ í™œì„± ìƒíƒœë¼ë©´ ë‹«ê¸°\n",
        "    if (currentActiveCard === cardElement) {\n",
        "        similarSectionDiv.style.display = 'none';\n",
        "        similarStatusDiv.innerText = '(í´ë¦­í•˜ì—¬ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ)';\n",
        "        cardElement.classList.remove('active');\n",
        "        currentActiveCard = null;\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // 2. ë‹¤ë¥¸ ì¹´ë“œê°€ í™œì„± ìƒíƒœì˜€ë‹¤ë©´ ë‹«ê¸°\n",
        "    if (currentActiveCard) {\n",
        "        currentActiveCard.querySelector('.similar-news-section').style.display = 'none';\n",
        "        currentActiveCard.querySelector('.similar-status').innerText = '(í´ë¦­í•˜ì—¬ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ)';\n",
        "        currentActiveCard.classList.remove('active');\n",
        "    }\n",
        "\n",
        "    // 3. ë¡œë”© ì‹œì‘ ë° ìƒíƒœ í‘œì‹œ\n",
        "    currentActiveCard = cardElement;\n",
        "    similarSectionDiv.innerHTML = '';\n",
        "    similarStatusDiv.innerHTML = '<span class=\"loading-spinner\"></span> ìœ ì‚¬ ë‰´ìŠ¤ ì°¾ëŠ” ì¤‘... (ì•½ 5~15ì´ˆ ì†Œìš”)';\n",
        "    similarStatusDiv.style.color = '#007bff';\n",
        "    cardElement.classList.add('active');\n",
        "\n",
        "    try {\n",
        "        const resp = await fetch(`/api/similar_news?link=${encodeURIComponent(link)}&category=${encodeURIComponent(category)}`);\n",
        "        const data = await resp.json();\n",
        "\n",
        "        if (data.error) {\n",
        "            similarStatusDiv.innerText = `ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: ${data.error}`;\n",
        "            similarStatusDiv.style.color = '#c0392b';\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // ìƒì„¸ ìš”ì•½ì„ ê¸°ì¡´ ì¹´ë“œì— ì—…ë°ì´íŠ¸ (APIì—ì„œ ìƒˆë¡œ ë°›ì•„ì˜¨ ìƒì„¸ ìš”ì•½ìœ¼ë¡œ êµì²´)\n",
        "        const detailedSummaryValueSpan = cardElement.querySelector('.detailed-summary-text-value');\n",
        "        const newSummary = data.detailed_summary;\n",
        "        // ì„œë²„ì—ì„œ ë°˜í™˜ë  ìˆ˜ ìˆëŠ” ì‹¤íŒ¨ ë©”ì‹œì§€ (Python ì½”ë“œ ì°¸ê³ )\n",
        "        const failedMessage1 = \"ê¸°ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\";\n",
        "        const failedMessage2 = \"ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒì„¸ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\";\n",
        "\n",
        "        // âš¡ï¸ Fix 1: ìƒˆë¡œ ë°›ì•„ì˜¨ ìš”ì•½ì´ ì‹¤íŒ¨ ë©”ì‹œì§€ì¸ ê²½ìš° ê¸°ì¡´ ìš”ì•½ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
        "        if (detailedSummaryValueSpan && !newSummary.includes(failedMessage1) && !newSummary.includes(failedMessage2)) {\n",
        "            detailedSummaryValueSpan.textContent = newSummary;\n",
        "        }\n",
        "        // ì‹¤íŒ¨ ë©”ì‹œì§€ì¸ ê²½ìš°: ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šì•„ ê¸°ì¡´ ìš”ì•½ì„ ìœ ì§€\n",
        "\n",
        "        // ìœ ì‚¬ ë‰´ìŠ¤ í‘œì‹œ\n",
        "        renderSimilarNews(data.similar_news, cardElement);\n",
        "        similarStatusDiv.innerText = 'ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ';\n",
        "        similarStatusDiv.style.color = '#2ecc71';\n",
        "\n",
        "    } catch (e) {\n",
        "        similarStatusDiv.innerText = 'ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ (ì„œë²„ ì—°ê²° ì˜¤ë¥˜)';\n",
        "        similarStatusDiv.style.color = '#c0392b';\n",
        "        console.error('ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨:', e);\n",
        "    }\n",
        "}\n",
        "\n",
        "function renderNews(data) {\n",
        "    const container = document.getElementById('newsContainer');\n",
        "    container.innerHTML = '';\n",
        "    if (!Array.isArray(data) || !data.length) {\n",
        "        container.innerText = 'ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.';\n",
        "        return;\n",
        "    }\n",
        "    const grid = document.createElement('div');\n",
        "    grid.className = 'news-grid';\n",
        "\n",
        "    data.forEach(n => {\n",
        "        const card = document.createElement('div');\n",
        "        card.className = 'news-card';\n",
        "\n",
        "        // í´ë¦­ ì´ë²¤íŠ¸ ì„¤ì •\n",
        "        card.onclick = () => fetchSimilarNews(n.link, n.category, card);\n",
        "\n",
        "        const img = document.createElement('img');\n",
        "        img.src = n.image_url || 'https://via.placeholder.com/400x200?text=No+Image';\n",
        "        card.appendChild(img);\n",
        "\n",
        "        const content = document.createElement('div');\n",
        "        content.className = 'content';\n",
        "\n",
        "        // âš¡ï¸ Fix 1: ìƒì„¸ë³´ê¸° í…ìŠ¤íŠ¸ë¥¼ <span>ìœ¼ë¡œ ê°ì‹¸ì„œ JavaScriptì—ì„œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ê²Œ í•¨\n",
        "        content.innerHTML = `\n",
        "            <h3>${n.title}</h3>\n",
        "            <p><b>ë¯¸ë¦¬ë³´ê¸°:</b> ${n.preview_summary}</p>\n",
        "            <p class=\"detailed-summary-container\"><b>ìƒì„¸ë³´ê¸°:</b> <span class=\"detailed-summary-text-value\">${n.detailed_summary}</span></p>\n",
        "            <div class=\"meta\">${n.pub_date || ''} | ${n.publisher || ''} | ${n.category || 'ê¸°íƒ€'}</div>\n",
        "            <a href=\"${n.link}\" target=\"_blank\" onclick=\"event.stopPropagation();\">ì›ë¬¸ ë³´ê¸°</a>\n",
        "            <div class=\"similar-status\" style=\"font-size:12px;margin-top:5px; height: 16px;\">(í´ë¦­í•˜ì—¬ ìœ ì‚¬ ë‰´ìŠ¤ ë¡œë“œ)</div>\n",
        "            <div class=\"similar-news-section\" style=\"display:none;\"></div>\n",
        "        `;\n",
        "        // **********************************************\n",
        "        card.appendChild(content);\n",
        "        grid.appendChild(card);\n",
        "    });\n",
        "    container.appendChild(grid);\n",
        "}\n",
        "\n",
        "function renderSimilarNews(data, cardElement) {\n",
        "    const similarSection = cardElement.querySelector('.similar-news-section');\n",
        "    similarSection.style.display = 'block';\n",
        "\n",
        "    if (data && data.length > 0) {\n",
        "        let similarHtml = '<h4>ìœ ì‚¬ ì¶”ì²œ ë‰´ìŠ¤:</h4><div class=\"similar-news-grid\">';\n",
        "        data.forEach(similar => {\n",
        "            // ìœ ì‚¬ ì¶”ì²œ ë‰´ìŠ¤ë„ ìš”ì•½ê³¼ í•¨ê»˜ í‘œì‹œ\n",
        "            // ìœ ì‚¬ë„ ì ìˆ˜ (similarity_score) ì¶”ê°€ í‘œì‹œ\n",
        "            const score = similar.similarity_score ? `[ìœ ì‚¬ë„: ${similar.similarity_score}]` : '';\n",
        "            similarHtml += `\n",
        "                <div class=\"similar-news-card\">\n",
        "                    <h5>${similar.title} ${score}</h5>\n",
        "                    <p>${similar.preview_summary}</p>\n",
        "                    <div class=\"meta\">${similar.publisher} | <a href=\"${similar.link}\" target=\"_blank\" onclick=\"event.stopPropagation();\">ì›ë¬¸</a></div>\n",
        "                </div>\n",
        "            `;\n",
        "        });\n",
        "        similarHtml += '</div>';\n",
        "        similarSection.innerHTML = similarHtml;\n",
        "    } else {\n",
        "        similarSection.innerHTML = '<p>ì¶”í›„ ê°œë°œ ì˜ˆì •..</p>';\n",
        "    }\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "    \"\"\"\n",
        "    return render_template_string(TEST_PAGE_HTML)\n",
        "\n",
        "# -----------------------------\n",
        "# ì„œë²„ ì‹¤í–‰\n",
        "# -----------------------------\n",
        "def run_app():\n",
        "    app.run(host='0.0.0.0', port=8000, debug=False, use_reloader=False)\n",
        "\n",
        "if NGROK_AUTHTOKEN:\n",
        "    os.system(f\"ngrok config add-authtoken {NGROK_AUTHTOKEN}\")\n",
        "    server_thread = threading.Thread(target=run_app, daemon=True)\n",
        "    server_thread.start()\n",
        "    time.sleep(2)\n",
        "    try:\n",
        "        public_url = ngrok.connect(8000).public_url\n",
        "        print(f\"\\nğŸ‰ ì„œë²„ ì‹¤í–‰! ì ‘ì†: {public_url}/test\")\n",
        "    except: pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTvoVye6nWdf",
        "outputId": "7fca2509-beec-46c4-e2c8-21ae1ad3b776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.12/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.32.4)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.12)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2025.10.5)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.0)\n",
            "ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 8000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ ì„œë²„ ì‹¤í–‰! ì ‘ì†: https://3f1a2a527384.ngrok-free.app/test\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}